from fastapi import FastAPI, HTTPException, WebSocket
from pydantic import BaseModel
import structlog
import os
import json
import asyncio
from datetime import datetime
from typing import List, Optional, Dict, Any
import uuid
import sys
from contextlib import asynccontextmanager

# Add current directory to Python path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# Hardcoded logic prevention removed - was causing more issues than benefits
# Check if running in Docker container
if os.path.exists('/app/shared'):
    sys.path.append('/app/shared')
else:
    sys.path.append('/home/opsconductor/opsconductor-ng/shared')
from base_service import BaseService

# Import the simplified intent brain system
from brains.intent_brain.intent_brain import IntentBrain

# Legacy brain engine COMPLETELY REMOVED - NO FALLBACKS ALLOWED

# LLM Engine import
from integrations.llm_client import LLMEngine

# PURE LLM CHAT HANDLER - EMBEDDED TO AVOID IMPORT ISSUES

# Modern API imports (replacing legacy)
# from api.knowledge_router import knowledge_router  # Temporarily disabled - missing knowledge_engine
# from api.learning_router import learning_router  # Temporarily disabled - missing knowledge_engine

# Legacy analytics and processing REMOVED - Multi-Brain AI Engine handles all processing

# Job Engine imports
from job_engine.workflow_generator import WorkflowGenerator

# Fulfillment Engine imports
from fulfillment_engine.fulfillment_engine import FulfillmentEngine, FulfillmentRequest, FulfillmentStatus
from fulfillment_engine.fulfillment_orchestrator import FulfillmentOrchestrator
from fulfillment_engine.direct_executor import DirectExecutor

# Orchestration Engine imports
from orchestration.ai_brain_service import AIBrainService
from orchestration.prefect_flow_engine import PrefectFlowEngine

# Integration imports
from integrations.asset_client import AssetServiceClient
from integrations.automation_client import AutomationServiceClient
from integrations.network_client import NetworkAnalyzerClient
from integrations.communication_client import CommunicationServiceClient
from integrations.prefect_client import PrefectClient

# Streaming infrastructure imports
from streaming.stream_manager import initialize_global_stream_manager, get_global_stream_manager
from api.thinking_websocket import initialize_websocket_manager, thinking_websocket_endpoint, progress_websocket_endpoint

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

def create_lifespan(service_instance):
    @asynccontextmanager
    async def lifespan(app: FastAPI):
        """Initialize AI engine on startup and cleanup on shutdown"""
        print("ðŸš€ AI BRAIN STARTUP EVENT CALLED!")
        logger.info("ðŸš€ AI BRAIN STARTUP EVENT CALLED!")
        
        print("ðŸ”§ About to call base service startup...")
        logger.info("ðŸ”§ About to call base service startup...")
        
        # Initialize base service components (database, redis) first
        await service_instance._startup()
        logger.info("Base service components initialized")
        
        try:
            
            print(f"ðŸ”— About to initialize LLM engine: {llm_engine}")
            logger.info(f"ðŸ”— About to initialize LLM engine: {llm_engine}")
            
            # Initialize LLM engine first
            print("ðŸ”— Initializing LLM engine...")
            logger.info("ðŸ”— Initializing LLM engine...")
            
            print(f"ðŸ”§ LLM engine type: {type(llm_engine)}")
            print(f"ðŸ”§ LLM engine host: {llm_engine.ollama_host}")
            print(f"ðŸ”§ LLM engine default model: {llm_engine.default_model}")
            
            # Test Ollama connection directly first
            try:
                print("ðŸ”§ Testing direct Ollama connection...")
                import ollama
                test_client = ollama.Client(host=llm_engine.ollama_host)
                test_response = test_client.list()
                print(f"ðŸ”§ Direct Ollama test successful: {len(test_response.models)} models")
            except Exception as e:
                print(f"ðŸ”§ Direct Ollama test failed: {e}")
            
            try:
                llm_success = await llm_engine.initialize()
                
                if llm_success:
                    logger.info("ðŸš€ LLM engine initialized successfully")
                else:
                    logger.error("âŒ Failed to initialize LLM engine")
            except Exception as e:
                logger.error(f"âŒ LLM engine initialization failed with exception: {e}")
                llm_success = False
            
            # Intent brain is ready (no initialization needed)
            logger.info("ðŸš€ Intent Brain System ready")
            logger.info("ðŸ§  Intent analysis and job creation ready")
            
            # Initialize AI Brain Orchestration Service
            logger.info("ðŸŽ¼ Initializing AI Brain Orchestration Service...")
            orchestration_success = await ai_brain_service.initialize()
            if orchestration_success:
                logger.info("ðŸš€ AI Brain Orchestration Service initialized successfully")
            else:
                logger.warning("âš ï¸ AI Brain Orchestration Service initialization failed - continuing without orchestration")
            
            # Initialize Prefect Flow Engine
            logger.info("ðŸŒŠ Initializing Prefect Flow Engine...")
            prefect_success = await prefect_flow_engine.initialize()
            if prefect_success:
                logger.info("ðŸš€ Prefect Flow Engine initialized successfully")
            else:
                logger.warning("âš ï¸ Prefect Flow Engine initialization failed - continuing without Prefect")
            
            # Set orchestration services for API router
            try:
                set_orchestration_services(ai_brain_service, prefect_flow_engine)
                logger.info("ðŸ”— Orchestration services linked to API router")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to link orchestration services to API: {e}")
            
            # Initialize streaming infrastructure
            logger.info("ðŸ“¡ Initializing streaming infrastructure...")
            redis_url = os.getenv("REDIS_URL", "redis://redis:6379/9")
            logger.info(f"ðŸ”— Using Redis URL: {redis_url}")
            streaming_success = await initialize_global_stream_manager(redis_url)
            if streaming_success:
                logger.info("ðŸš€ Streaming infrastructure initialized successfully")
                
                # Initialize WebSocket manager
                stream_manager = get_global_stream_manager()
                if stream_manager:
                    initialize_websocket_manager(stream_manager.redis_stream_manager)
                    logger.info("ðŸ”Œ WebSocket manager initialized")
            else:
                logger.warning("âš ï¸ Streaming infrastructure initialization failed - continuing without real-time features")
        except Exception as e:
            print(f"âŒ AI BRAIN STARTUP EXCEPTION: {e}")
            logger.error(f"âŒ Exception during AI Brain startup: {e}")
            import traceback
            traceback.print_exc()
        
        yield  # This is where the app runs
        
        # Shutdown AI Brain components
        logger.info("Shutting down AI Brain System...")
        try:
            # Cleanup Intent Brain
            if hasattr(intent_brain, 'cleanup'):
                await intent_brain.cleanup()
                logger.info("ðŸ§  Intent Brain System cleanup completed")
            else:
                logger.info("ðŸ§  Intent Brain System shutdown completed")
            
            # Cleanup Prefect Flow Engine
            if hasattr(prefect_flow_engine, 'cleanup'):
                await prefect_flow_engine.cleanup()
                logger.info("ðŸŒŠ Prefect Flow Engine cleanup completed")
            
            # Cleanup AI Brain Orchestration Service
            if hasattr(ai_brain_service, 'cleanup'):
                await ai_brain_service.cleanup()
                logger.info("ðŸŽ¼ AI Brain Orchestration Service cleanup completed")
            
            # Cleanup streaming infrastructure
            stream_manager = get_global_stream_manager()
            if stream_manager:
                await stream_manager.shutdown()
                logger.info("ðŸ“¡ Streaming infrastructure cleanup completed")
                
        except Exception as e:
            logger.error(f"âŒ Exception during AI Brain System shutdown: {e}")
        
        # Finally, shutdown base service components
        await service_instance._shutdown()
        logger.info("Base service components shutdown")
    
    return lifespan

class AIService(BaseService):
    def __init__(self):
        # Create the lifespan function with self reference
        lifespan_func = create_lifespan(self)
        super().__init__(
            name="ai-service",
            version="2.0.0-HARDCODE-FREE",
            port=3005,
            lifespan=lifespan_func
        )
        
# Hardcoded logic prevention removed - was blocking legitimate functionality

# Initialize service components - must be after lifespan function definition
service = AIService()
app = service.app

# Include modern API routes
# app.include_router(knowledge_router)  # Temporarily disabled - missing knowledge_engine
# app.include_router(learning_router)  # Temporarily disabled - missing knowledge_engine

# Include Prefect integration router
try:
    from api.prefect_router import prefect_router
    app.include_router(prefect_router)
    logger.info("âœ… Prefect API router included")
except ImportError as e:
    logger.warning(f"âš ï¸  Prefect router not available: {str(e)}")

# Include Orchestration router
try:
    from api.orchestration_router import orchestration_router, set_orchestration_services
    app.include_router(orchestration_router)
    logger.info("âœ… Orchestration API router included")
except ImportError as e:
    logger.warning(f"âš ï¸  Orchestration router not available: {str(e)}")

# Include Streaming router
try:
    from api.streaming_router import router as streaming_router
    app.include_router(streaming_router)
    logger.info("âœ… Streaming API router included")
except ImportError as e:
    logger.warning(f"âš ï¸  Streaming router not available: {str(e)}")

# Include Thinking LLM router
try:
    from api.thinking_llm_router import router as thinking_llm_router
    app.include_router(thinking_llm_router)
    logger.info("âœ… Thinking LLM API router included")
except ImportError as e:
    logger.warning(f"âš ï¸  Thinking LLM router not available: {str(e)}")

# Initialize LLM Engine
ollama_host = os.getenv("OLLAMA_HOST", "http://ollama:11434")
default_model = os.getenv("DEFAULT_MODEL", "codellama:7b")
llm_engine = LLMEngine(ollama_host, default_model)

# Initialize simplified intent brain system
workflow_generator = WorkflowGenerator()
asset_client = AssetServiceClient(os.getenv("ASSET_SERVICE_URL", "http://asset-service:3002"))
automation_client = AutomationServiceClient(os.getenv("AUTOMATION_SERVICE_URL", "http://automation-service:3003"))
network_client = NetworkAnalyzerClient(os.getenv("NETWORK_ANALYZER_URL", "http://network-analyzer-service:3006"))
communication_client = CommunicationServiceClient(os.getenv("COMMUNICATION_SERVICE_URL", "http://communication-service:3004"))
prefect_client = PrefectClient()

logger.info("ðŸ§  Initializing Intent Brain System")
intent_brain = IntentBrain(llm_engine)

# Initialize Fulfillment Engine and Orchestrator
logger.info("ðŸŽ¯ Initializing Fulfillment Engine")
fulfillment_engine = FulfillmentEngine(
    llm_engine=llm_engine,
    automation_client=automation_client,
    asset_client=asset_client
)

logger.info("ðŸŽ¯ Initializing Fulfillment Orchestrator")
fulfillment_orchestrator = FulfillmentOrchestrator(
    llm_engine=llm_engine,
    automation_client=automation_client,
    asset_client=asset_client,
    network_client=network_client
)

logger.info("ðŸš€ Initializing Direct Executor - OLLAMA MAKES ALL DECISIONS")
direct_executor = DirectExecutor(
    llm_engine=llm_engine,
    automation_client=automation_client,
    asset_client=asset_client,
    network_client=network_client,
    communication_client=communication_client,
    prefect_client=prefect_client
)

logger.info("ðŸŽ¼ Initializing AI Brain Orchestration Service")
ai_brain_service = AIBrainService()

logger.info("ðŸŒŠ Initializing Prefect Flow Engine")
prefect_flow_engine = PrefectFlowEngine()

# Add WebSocket routes for real-time thinking visualization
@app.websocket("/ws/thinking/{session_id}")
async def thinking_websocket(websocket: WebSocket, session_id: str):
    """WebSocket endpoint for real-time thinking visualization"""
    await thinking_websocket_endpoint(websocket, session_id)

@app.websocket("/ws/progress/{session_id}")
async def progress_websocket(websocket: WebSocket, session_id: str):
    """WebSocket endpoint for real-time progress updates"""
    await progress_websocket_endpoint(websocket, session_id)

@app.post("/api/v1/init-streaming")
async def manual_init_streaming():
    """Manual endpoint to initialize streaming infrastructure"""
    try:
        logger.info("ðŸ“¡ Manual streaming infrastructure initialization...")
        redis_url = os.getenv("REDIS_URL", "redis://redis:6379/9")
        logger.info(f"ðŸ”— Using Redis URL: {redis_url}")
        
        streaming_success = await initialize_global_stream_manager(redis_url)
        if streaming_success:
            logger.info("ðŸš€ Streaming infrastructure initialized successfully")
            
            # Initialize WebSocket manager
            stream_manager = get_global_stream_manager()
            if stream_manager:
                initialize_websocket_manager(stream_manager.redis_stream_manager)
                logger.info("ðŸ”Œ WebSocket manager initialized")
                return {"status": "success", "message": "Streaming infrastructure initialized"}
            else:
                return {"status": "error", "message": "Stream manager not available"}
        else:
            return {"status": "error", "message": "Failed to initialize streaming infrastructure"}
    except Exception as e:
        logger.error(f"âŒ Manual initialization failed: {e}")
        return {"status": "error", "message": f"Initialization failed: {str(e)}"}

# Old startup/shutdown events removed - now using lifespan context manager

# PURE AI BRAIN DECISION MAKING - NO HARDCODED LOGIC!
async def pure_llm_chat_endpoint(request, intent_brain_instance):
    """
    PURE AI BRAIN DECISION MAKING WITH REAL-TIME THINKING VISUALIZATION
    
    The AI Brain (Ollama) makes ALL decisions about:
    - Which services to use
    - How to process the request
    - What response format to return
    - Everything else!
    
    NO HARDCODED LOGIC, NO PATTERN MATCHING, NO FALLBACKS!
    THE AI BRAIN DECIDES EVERYTHING!
    
    PLUS: Real-time thinking stream for debug mode visualization
    """
    try:
        logger.info("ðŸ§  AI BRAIN MAKING ALL DECISIONS", message=request.message[:100])
        
        # Generate conversation_id if not provided
        if not request.conversation_id:
            request.conversation_id = f"chat-{uuid.uuid4()}"
        
        # Create thinking session for real-time visualization
        thinking_session_id = f"thinking_{int(datetime.now().timestamp() * 1000)}_{uuid.uuid4().hex[:8]}"
        
        # Get stream manager for thinking visualization
        stream_manager = get_global_stream_manager()
        if stream_manager:
            # Create thinking session
            try:
                await stream_manager.create_thinking_session(
                    session_id=thinking_session_id,
                    user_id=str(request.user_id) if request.user_id else "system",
                    debug_mode=True,  # Always enable for real-time visualization
                    user_request=request.message[:100],
                    system_context={
                        "conversation_id": request.conversation_id,
                        "message_preview": request.message[:100]
                    }
                )
                
                # Stream initial analysis step
                from streaming.thinking_data_models import ThinkingType
                await stream_manager.stream_thinking(
                    session_id=thinking_session_id,
                    thinking_type=ThinkingType.ANALYSIS,
                    content="Analyzing user request and determining optimal processing approach...",
                    reasoning_chain=[
                        "Examining user message for intent and complexity",
                        "Reviewing available system services and capabilities",
                        "Checking conversation history for context",
                        "Determining optimal AI processing strategy"
                    ],
                    confidence=0.9,
                    alternatives=["Direct response", "Service orchestration", "Workflow generation"],
                    decision_factors=["Message complexity", "Available services", "User context"]
                )
            except Exception as e:
                logger.warning(f"Failed to create thinking session: {e}")
                stream_manager = None
        
        user_context = {
            "user_id": str(request.user_id) if request.user_id else "system",
            "conversation_id": request.conversation_id,
            "thinking_session_id": thinking_session_id,
            "available_services": {
                "ai_brain_orchestration": ai_brain_service and ai_brain_service.initialization_complete,
                "prefect_flow_engine": prefect_flow_engine is not None,
                "direct_executor": direct_executor is not None,
                "fulfillment_engine": fulfillment_engine is not None
            }
        }
        
        # Stream decision step
        if stream_manager:
            try:
                await stream_manager.stream_thinking(
                    session_id=thinking_session_id,
                    thinking_type=ThinkingType.DECISION,
                    content="Selecting optimal processing approach based on analysis...",
                    reasoning_chain=[
                        "Evaluating message complexity and requirements",
                        "Matching requirements to available service capabilities",
                        "Selecting most appropriate processing engine",
                        "Preparing execution context and parameters"
                    ],
                    confidence=0.85,
                    decision_factors=["Service availability", "Processing complexity", "Response requirements"]
                )
            except Exception as e:
                logger.warning(f"Failed to stream decision step: {e}")
        
        # THE AI BRAIN DECIDES EVERYTHING!
        # Pass all available services and let Ollama choose what to do
        execution_result = await direct_executor.execute_user_request_with_full_control(
            message=request.message, 
            user_context=user_context,
            available_services={
                "ai_brain_service": ai_brain_service,
                "prefect_flow_engine": prefect_flow_engine,
                "fulfillment_engine": fulfillment_engine
            }
        )
        
        # Stream completion step
        if stream_manager:
            try:
                await stream_manager.stream_thinking(
                    session_id=thinking_session_id,
                    thinking_type=ThinkingType.REFLECTION,
                    content="Processing completed successfully. Reviewing results and preparing response...",
                    reasoning_chain=[
                        "Validating execution results and response quality",
                        "Ensuring all user requirements have been addressed",
                        "Preparing final response with appropriate context",
                        "Completing thinking session and cleanup"
                    ],
                    confidence=execution_result.get("confidence", 0.8) if isinstance(execution_result, dict) else 0.8,
                    decision_factors=["Result quality", "User satisfaction", "System performance"]
                )
                
                # Close thinking session
                await stream_manager.close_session(thinking_session_id)
            except Exception as e:
                logger.warning(f"Failed to stream completion step: {e}")
        
        # Add thinking session ID to response for frontend tracking
        if isinstance(execution_result, dict):
            execution_result["thinking_session_id"] = thinking_session_id
        
        # Return exactly what the AI Brain decided - NO MODIFICATIONS!
        return execution_result
        
    except Exception as e:
        logger.error("âŒ AI BRAIN DECISION MAKING FAILED", error=str(e), exc_info=True)
        
        # Even error handling is decided by the AI Brain
        try:
            error_context = {
                "error": str(e),
                "user_id": str(request.user_id) if request.user_id else "system",
                "conversation_id": request.conversation_id,
                "original_message": request.message
            }
            
            # Let AI Brain handle the error
            error_result = await direct_executor.handle_error_with_ai_decision(error_context)
            return error_result
            
        except Exception as final_error:
            logger.error("âŒ EVEN AI BRAIN ERROR HANDLING FAILED", error=str(final_error))
            # Only as absolute last resort
            return {
                "response": "I'm experiencing technical difficulties. Please try again.",
                "conversation_id": request.conversation_id,
                "ai_brain_decision": "emergency_fallback"
            }

# NO MORE HARDCODED FORMATTING FUNCTIONS!
# THE AI BRAIN DECIDES EVERYTHING!

# Removed all hardcoded formatting functions:
# - _format_execution_response
# - _format_orchestration_response  
# - _format_ollama_response
# The AI Brain now handles all response formatting decisions!

# All formatting functions removed - AI Brain handles everything!

# All formatting functions removed - AI Brain handles everything!

# Broken function removed - proper one exists later in file

async def _analyze_user_intent_with_llm(message: str, intent_brain_instance) -> Dict[str, Any]:
    """Use LLM to analyze if the user wants to create a job or have a conversation"""
    try:
        # Use the global LLM engine
        if llm_engine is None:
            logger.error("ðŸ” LLM engine not available - NO FALLBACK ALLOWED")
            # NO FALLBACK - FAIL HARD AS REQUESTED
            raise Exception("LLM engine not available - SYSTEM CANNOT FUNCTION WITHOUT AI INTENT ANALYSIS")
        
        analysis_prompt = f"""Analyze this user message and determine if they want to create an automation job or just have a conversation.

User message: "{message}"

Please consider whether the user is asking you to perform an action or execute something, versus asking for information or having a discussion.

Please respond naturally by answering these questions:
1. Is this a job request or conversation?
2. How confident are you (high/medium/low)?
3. What type is it (if job: automation/deployment/monitoring/maintenance/query, if conversation: question/help/general)?
4. Why do you think so?

Some examples to help guide your analysis:
- "restart nginx service on server1" â†’ likely a job request (performing an action)
- "run echo hello on localhost" â†’ likely a job request (executing a command)  
- "how does nginx work?" â†’ likely a conversation (asking for information)
- "what is the status of server1?" â†’ could be either (depends on context - asking for info vs requesting a status check)

Use your best judgment and explain your reasoning in natural language."""

        llm_response = await llm_engine.generate(analysis_prompt)
        logger.info(f"ðŸ” LLM intent analysis response: {llm_response}")
        
        # Extract the generated text from the LLM response
        if isinstance(llm_response, dict) and "generated_text" in llm_response:
            generated_text = llm_response["generated_text"]
        else:
            generated_text = str(llm_response)
        
        logger.info(f"ðŸ” Extracted generated text: {generated_text}")
        
        # Parse natural language response
        try:
            analysis = _parse_natural_language_intent_analysis(generated_text)
            logger.info(f"ðŸ” Parsed intent analysis: {analysis}")
            return analysis
        except Exception as e:
            logger.error(f"ðŸ” Natural language parsing failed: {e}")
            raise Exception(f"LLM response parsing FAILED: {e}")
            
    except Exception as e:
        logger.error(f"âŒ Intent analysis failed: {e}")
        # NO FALLBACK - FAIL HARD AS REQUESTED
        raise Exception(f"Intent analysis COMPLETELY FAILED - NO FALLBACK ALLOWED: {e}")

def _parse_natural_language_intent_analysis(response_text: str) -> Dict[str, Any]:
    """Parse natural language intent analysis response from LLM"""
    response_lower = response_text.lower()
    
    # Determine if it's a job request
    is_job_request = False
    if any(phrase in response_lower for phrase in [
        "job request", "automation", "action", "execute", "perform", "run", "deploy", "install", "restart", "start", "stop"
    ]):
        is_job_request = True
    elif any(phrase in response_lower for phrase in [
        "conversation", "question", "information", "help", "discuss", "explain", "what is", "how does"
    ]):
        is_job_request = False
    else:
        # Default logic based on keywords
        action_keywords = ["restart", "run", "execute", "deploy", "install", "configure", "setup", "create", "delete", "update", "ping", "check status"]
        conversation_keywords = ["what", "how", "why", "explain", "tell me", "help", "information"]
        
        action_count = sum(1 for keyword in action_keywords if keyword in response_lower)
        conversation_count = sum(1 for keyword in conversation_keywords if keyword in response_lower)
        
        is_job_request = action_count > conversation_count
    
    # Determine confidence
    confidence = 0.8  # Default medium-high confidence
    if any(phrase in response_lower for phrase in ["high confidence", "very confident", "certain", "definitely"]):
        confidence = 0.95
    elif any(phrase in response_lower for phrase in ["medium confidence", "somewhat confident", "likely"]):
        confidence = 0.7
    elif any(phrase in response_lower for phrase in ["low confidence", "uncertain", "not sure", "maybe"]):
        confidence = 0.5
    
    # Determine job type or conversation type
    job_type = None
    conversation_type = None
    
    if is_job_request:
        if any(word in response_lower for word in ["deploy", "install", "setup"]):
            job_type = "deployment"
        elif any(word in response_lower for word in ["monitor", "check", "status", "ping"]):
            job_type = "monitoring"
        elif any(word in response_lower for word in ["maintain", "update", "patch", "cleanup"]):
            job_type = "maintenance"
        elif any(word in response_lower for word in ["query", "list", "show", "get"]):
            job_type = "query"
        else:
            job_type = "automation"
    else:
        if any(word in response_lower for word in ["help", "how to", "guide"]):
            conversation_type = "help"
        elif any(word in response_lower for word in ["what", "explain", "tell me"]):
            conversation_type = "question"
        else:
            conversation_type = "general"
    
    # Extract reasoning (look for "because", "since", "reason", etc.)
    reasoning = "Based on natural language analysis of user intent"
    reasoning_indicators = ["because", "since", "reason", "why", "this is"]
    for indicator in reasoning_indicators:
        if indicator in response_lower:
            # Try to extract the reasoning part
            parts = response_text.split(indicator, 1)
            if len(parts) > 1:
                reasoning = parts[1].strip()[:200]  # Limit length
                break
    
    return {
        "is_job_request": is_job_request,
        "confidence": confidence,
        "reasoning": reasoning,
        "job_type": job_type,
        "conversation_type": conversation_type
    }

def _parse_natural_language_clarification_analysis(response_text: str) -> Dict[str, Any]:
    """Parse natural language clarification analysis response from LLM"""
    response_lower = response_text.lower()
    
    # Determine if it's a clarification response
    is_clarification_response = False
    if any(phrase in response_lower for phrase in [
        "clarification", "additional information", "responding to", "answering", "providing details", "follow-up"
    ]):
        is_clarification_response = True
    elif any(phrase in response_lower for phrase in [
        "new request", "different request", "unrelated", "separate", "not clarification"
    ]):
        is_clarification_response = False
    else:
        # Look for patterns that suggest clarification
        clarification_indicators = [
            "every", "at", "when", "if", "above", "below", "less than", "more than", 
            "yes", "no", "daily", "hourly", "weekly", "monthly", "midnight", "noon"
        ]
        clarification_count = sum(1 for indicator in clarification_indicators if indicator in response_lower)
        is_clarification_response = clarification_count > 0
    
    # Determine confidence
    confidence = 0.7  # Default medium confidence
    if any(phrase in response_lower for phrase in ["high confidence", "very confident", "certain", "definitely"]):
        confidence = 0.9
    elif any(phrase in response_lower for phrase in ["medium confidence", "somewhat confident", "likely"]):
        confidence = 0.7
    elif any(phrase in response_lower for phrase in ["low confidence", "uncertain", "not sure", "maybe"]):
        confidence = 0.4
    
    # Extract reasoning
    reasoning = "Based on natural language analysis of clarification patterns"
    reasoning_indicators = ["because", "since", "reason", "why", "this is", "appears to be"]
    for indicator in reasoning_indicators:
        if indicator in response_lower:
            parts = response_text.split(indicator, 1)
            if len(parts) > 1:
                reasoning = parts[1].strip()[:200]
                break
    
    return {
        "is_clarification_response": is_clarification_response,
        "confidence": confidence,
        "reasoning": reasoning
    }

async def _handle_job_creation_request(request, intent_brain_instance, intent_analysis) -> Dict[str, Any]:
    """Handle job creation requests - DIRECT OLLAMA EXECUTION"""
    try:
        logger.info("ðŸš€ DIRECT OLLAMA EXECUTION - NO COMPLEX PIPELINES!")
        
        user_context = {
            "user_id": str(request.user_id) if request.user_id else "system",
            "conversation_id": request.conversation_id
        }
        
        # ðŸš€ DIRECT EXECUTION: Let Ollama decide everything and execute it!
        logger.info("ðŸ§  Ollama will analyze, plan, and execute directly")
        execution_result = await direct_executor.execute_user_request(request.message, user_context)
        
        # Build response based on execution result
        if execution_result.get("status") == "completed":
            response_text = f"âœ… TASK COMPLETED SUCCESSFULLY\n\n{execution_result.get('message', 'Task completed')}"
            execution_started = True
            confidence = 1.0
        elif execution_result.get("status") == "failed":
            response_text = f"âŒ TASK EXECUTION FAILED\n\n{execution_result.get('message', 'Task failed')}"
            execution_started = False
            confidence = 0.5
        else:
            response_text = f"â³ TASK IN PROGRESS\n\n{execution_result.get('message', 'Task running')}"
            execution_started = True
            confidence = 0.8
        
        # Extract job details if available
        job_details = execution_result.get("job_details", [])
        primary_job_id = None
        primary_execution_id = None
        
        if job_details:
            primary_job = job_details[0]
            primary_job_id = primary_job.get('job_id')
            primary_execution_id = primary_job.get('execution_id')
            
            # Add job details to response
            response_text += f"\n\nðŸ“‹ Job Details:\n"
            for job_detail in job_details:
                job_name = job_detail.get('job_name', 'AI Generated Job')
                job_id = job_detail.get('job_id', 'N/A')
                execution_id = job_detail.get('execution_id', 'N/A')
                response_text += f"â€¢ Job: {job_name} (ID: {job_id}, Execution: {execution_id})\n"
        
        return {
            "response": response_text,
            "conversation_id": request.conversation_id,
            "intent": "direct_execution",
            "confidence": confidence,
            "job_id": primary_job_id,
            "execution_id": primary_execution_id,
            "automation_job_id": primary_job_id,
            "workflow": {"execution_logs": [execution_result.get("execution_response", "")]},
            "execution_started": execution_started,
            "intent_classification": {
                "intent_type": "direct_ollama_execution",
                "confidence": confidence,
                "method": "direct_executor",
                "alternatives": [],
                "entities": [],
                "context_analysis": {
                    "confidence_score": confidence,
                    "risk_level": "LOW",
                    "requirements_count": 1,
                    "recommendations": ["Executed directly by Ollama"]
                },
                "reasoning": "Ollama analyzed and executed the request directly",
                "metadata": {
                    "engine": "direct_executor",
                    "success": execution_result.get("status") == "completed"
                }
            },
            "timestamp": datetime.utcnow().isoformat(),
            "_routing": {
                "service_type": "direct_ollama_executor", 
                "response_time": 0.0,
                "cached": False
            },
            "execution_result": execution_result,
            "job_details": job_details
        }
            
    except Exception as e:
        logger.error(f"âŒ Intent analysis failed: {e}")
        return {
            "response": f"ðŸŽ¯ INTENT ANALYSIS ERROR\n\nI encountered an error while analyzing your intent: {str(e)}",
            "conversation_id": request.conversation_id,
            "intent": "intent_analysis_error",
            "confidence": 0.0,
            "job_id": None,
            "execution_id": None,
            "automation_job_id": None,
            "workflow": None,
            "execution_started": False,
            "intent_classification": {
                "intent_type": "intent_analysis_error",
                "confidence": 0.0,
                "method": "intent_brain_analysis",
                "alternatives": [],
                "entities": [],
                "context_analysis": {"error": str(e)},
                "reasoning": "Exception in intent analysis",
                "metadata": {
                    "engine": "intent_brain_only",
                    "success": False,
                    "error": str(e)
                }
            },
            "timestamp": datetime.utcnow().isoformat(),
            "_routing": {
                "service_type": "intent_analysis_error", 
                "response_time": 0.0,
                "cached": False
            }
        }

async def _handle_clarification_needed(request, intent_analysis_result, intent_brain_instance) -> Dict[str, Any]:
    """Handle cases where clarifying questions are needed before proceeding"""
    try:
        logger.info("ðŸ¤” Generating clarification response for user")
        
        # Get the clarifying questions from the intent analysis
        clarifying_questions = intent_analysis_result.clarifying_questions
        missing_info = []  # Not used in simplified version
        
        # Generate a user-friendly clarification message
        clarification_message = await _generate_clarification_message(
            intent_analysis_result, clarifying_questions, missing_info, intent_brain_instance
        )
        
        # Store the partial analysis for follow-up
        conversation_context = {
            "partial_analysis": intent_analysis_result.to_dict() if hasattr(intent_analysis_result, 'to_dict') else str(intent_analysis_result),
            "original_message": request.message,
            "clarifying_questions": clarifying_questions,
            "missing_information": missing_info,
            "awaiting_clarification": True,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Store context globally for iterative clarification (in production, use Redis/database)
        _conversation_contexts[request.conversation_id] = conversation_context
        logger.info(f"ðŸ”„ Stored conversation context for {request.conversation_id}, awaiting clarification")
        
        return {
            "response": clarification_message,
            "conversation_id": request.conversation_id,
            "intent": "clarification_needed",
            "confidence": 1.0,
            "job_id": None,
            "execution_id": None,
            "automation_job_id": None,
            "workflow": None,
            "execution_started": False,
            "clarification_needed": True,
            "clarifying_questions": clarifying_questions,
            "missing_information": missing_info,
            "intent_classification": {
                "intent_type": "clarification_needed",
                "confidence": 1.0,
                "method": "intent_brain_clarification",
                "alternatives": [],
                "entities": [],
                "context_analysis": {
                    "confidence_score": 1.0,
                    "risk_level": "LOW",
                    "requirements_count": len(missing_info),
                    "recommendations": clarifying_questions
                },
                "reasoning": "Additional information needed to proceed",
                "metadata": {
                    "engine": "intent_brain_clarification",
                    "version": "2.0.0",
                    "success": True,
                    "clarification_context": conversation_context
                }
            },
            "timestamp": datetime.utcnow().isoformat(),
            "_routing": {
                "service_type": "clarification_needed", 
                "response_time": 0.0,
                "cached": False
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Clarification handling failed: {e}")
        return {
            "response": f"I need more information to proceed, but encountered an error generating clarifying questions: {str(e)}",
            "conversation_id": request.conversation_id,
            "intent": "clarification_error",
            "confidence": 0.0,
            "job_id": None,
            "execution_id": None,
            "automation_job_id": None,
            "workflow": None,
            "execution_started": False,
            "clarification_needed": True,
            "clarifying_questions": [],
            "missing_information": [],
            "intent_classification": {
                "intent_type": "clarification_error",
                "confidence": 0.0,
                "method": "clarification_error",
                "alternatives": [],
                "entities": [],
                "context_analysis": {"error": str(e)},
                "reasoning": "Error generating clarification",
                "metadata": {
                    "engine": "clarification_error",
                    "success": False,
                    "error": str(e)
                }
            },
            "timestamp": datetime.utcnow().isoformat(),
            "_routing": {
                "service_type": "clarification_error", 
                "response_time": 0.0,
                "cached": False
            }
        }

async def _generate_clarification_message(intent_analysis_result, clarifying_questions, missing_info, intent_brain_instance) -> str:
    """Generate a user-friendly clarification message using LLM"""
    try:
        if not hasattr(intent_brain_instance, 'llm_engine') or not intent_brain_instance.llm_engine:
            # Fallback to simple message if LLM not available
            questions_text = "\n".join([f"â€¢ {q}" for q in clarifying_questions])
            return f"I need some additional information to proceed:\n\n{questions_text}\n\nPlease provide these details so I can help you better."
        
        # Use LLM to generate a natural clarification message
        clarification_prompt = f"""Generate a friendly, helpful message asking the user for clarification. 

The user's request needs additional information before I can proceed. Here's what I understand so far and what I need:

Missing Information:
{chr(10).join([f"- {info}" for info in missing_info])}

Specific Questions to Ask:
{chr(10).join([f"- {q}" for q in clarifying_questions])}

Generate a natural, conversational message that:
1. Acknowledges what I understand from their request
2. Explains why I need more information
3. Asks the clarifying questions in a friendly way
4. Encourages them to provide the missing details

Keep it concise but helpful. Don't use JSON format - just return the message text."""

        llm_response = await intent_brain_instance.llm_engine.generate(clarification_prompt)
        
        # Extract the generated text
        if isinstance(llm_response, dict) and "generated_text" in llm_response:
            generated_text = llm_response["generated_text"]
        else:
            generated_text = str(llm_response)
        
        # Clean up the response
        clarification_message = generated_text.strip()
        
        # Fallback if LLM response is empty or too short
        if len(clarification_message) < 20:
            questions_text = "\n".join([f"â€¢ {q}" for q in clarifying_questions])
            clarification_message = f"I need some additional information to proceed:\n\n{questions_text}\n\nPlease provide these details so I can help you better."
        
        return clarification_message
        
    except Exception as e:
        logger.error(f"âŒ Failed to generate clarification message: {e}")
        # Fallback to simple message
        questions_text = "\n".join([f"â€¢ {q}" for q in clarifying_questions])
        return f"I need some additional information to proceed:\n\n{questions_text}\n\nPlease provide these details so I can help you better."

# Global conversation context storage (in production, use Redis/database)
_conversation_contexts = {}

async def _check_for_clarification_followup(request, intent_brain_instance) -> Optional[Dict[str, Any]]:
    """Check if this is a clarification response and combine with original context for iterative clarification"""
    try:
        conversation_id = request.conversation_id
        
        # Check if we have stored context for this conversation
        if conversation_id not in _conversation_contexts:
            logger.info(f"ðŸ”„ No stored context for conversation {conversation_id} - treating as new request")
            return None
            
        stored_context = _conversation_contexts[conversation_id]
        
        # Check if we're awaiting clarification for this conversation
        if not stored_context.get("awaiting_clarification", False):
            logger.info(f"ðŸ”„ Not awaiting clarification for conversation {conversation_id}")
            return None
        
        logger.info(f"ðŸ”„ Found stored context awaiting clarification for conversation {conversation_id}")
        
        # Use the LLM to determine if this is likely a clarification response
        if not hasattr(intent_brain_instance, 'llm_engine') or not intent_brain_instance.llm_engine:
            logger.warning("ðŸ”„ LLM not available for clarification detection - skipping")
            return None
        
        clarification_detection_prompt = f"""Analyze this user message to determine if it appears to be providing additional information or clarification.

User message: "{request.message}"

Previous context: The user was asked clarifying questions about: {stored_context.get('missing_information', [])}

Look for:
- Timing/scheduling information (every day, hourly, at midnight, etc.)
- Threshold/alert values (less than 10GB, above 80%, etc.)
- Configuration details or parameters
- Short, direct answers that seem to be responding to questions

Does this look like a clarification response rather than a completely new request?

Please answer naturally:
1. Is this a clarification response or a new request?
2. How confident are you (high/medium/low)?
3. Why do you think so?

Explain your reasoning in natural language."""

        try:
            llm_response_data = await intent_brain_instance.llm_engine.generate(clarification_detection_prompt)
            llm_response = llm_response_data.get("generated_text", "")
            
            # Parse the natural language response
            analysis = _parse_natural_language_clarification_analysis(llm_response)
            
            if analysis.get("is_clarification_response", False) and analysis.get("confidence", 0) > 0.6:
                logger.info(f"ðŸ”„ Detected clarification response: {analysis.get('reasoning', 'No reasoning')}")
                
                # COMBINE ORIGINAL + CLARIFICATION FOR COMPLETE CONTEXT
                original_message = stored_context.get("original_message", "")
                combined_message = f"""{original_message}

Additional clarification provided: {request.message}"""
                
                logger.info(f"ðŸ”„ COMBINING CONTEXTS:")
                logger.info(f"ðŸ”„ Original: {original_message}")
                logger.info(f"ðŸ”„ Clarification: {request.message}")
                logger.info(f"ðŸ”„ Combined: {combined_message}")
                
                # Create a new request with the COMBINED context
                combined_request = type(request)(
                    message=combined_message,
                    conversation_id=request.conversation_id,
                    user_id=request.user_id
                )
                
                # Clear the awaiting clarification flag since we're processing it
                _conversation_contexts[conversation_id]["awaiting_clarification"] = False
                
                # Process the COMBINED request - this will re-analyze with full context
                # and may ask for MORE clarification if still needed (ITERATIVE!)
                logger.info(f"ðŸ”„ Re-analyzing COMBINED context for completeness...")
                return await _handle_job_creation_request(combined_request, intent_brain_instance, {"is_job_request": True})
            else:
                logger.info(f"ðŸ”„ Not a clarification response (confidence: {analysis.get('confidence', 0)}) - treating as new request")
                # Clear stored context since this seems to be a new request
                if conversation_id in _conversation_contexts:
                    del _conversation_contexts[conversation_id]
                return None
                
        except Exception as llm_error:
            logger.error(f"ðŸ”„ Clarification detection failed: {llm_error}")
            return None
        
    except Exception as e:
        logger.error(f"âŒ Error in clarification follow-up detection: {e}")
        return None

async def _generate_intent_interpretation(intent_analysis_result, intent_brain_instance) -> str:
    """Generate a human-readable interpretation of what the AI thinks the user wants"""
    try:
        if not hasattr(intent_brain_instance, 'llm_engine') or not intent_brain_instance.llm_engine:
            return "Intent analysis completed, but unable to generate human interpretation (LLM not available)"
        
        # Extract key information from intent analysis
        intent_data = intent_analysis_result.to_dict() if hasattr(intent_analysis_result, 'to_dict') else str(intent_analysis_result)
        
        interpretation_prompt = f"""Based on this intent analysis, explain in simple, human terms what you think the user wants to accomplish:

Intent Analysis Data:
{json.dumps(intent_data, indent=2, default=str)}

Provide a clear, conversational explanation that starts with "I understand that you want to..." and explains:
1. What the user is trying to accomplish
2. The key details or requirements identified
3. The confidence level of this understanding

Keep it concise but informative, as if explaining to the user what you understood from their request."""

        interpretation_result = await intent_brain_instance.llm_engine.generate(interpretation_prompt)
        
        # Extract the response text from the LLM result
        if isinstance(interpretation_result, dict):
            interpretation = interpretation_result.get('generated_text', str(interpretation_result))
        else:
            interpretation = str(interpretation_result)
            
        return interpretation.strip()
        
    except Exception as e:
        logger.error(f"Failed to generate intent interpretation: {e}")
        return f"Intent analysis completed. Raw data: {str(intent_analysis_result)[:200]}..."

async def _handle_conversation_request(request, intent_brain_instance, intent_analysis) -> Dict[str, Any]:
    """Handle conversation requests - FULL CONVERSATIONAL RESPONSE"""
    try:
        logger.info("ðŸ’¬ Generating conversational response using LLM")
        
        # Generate a proper conversational response using the LLM
        conversation_prompt = f"""You are a helpful AI assistant for OpsConductor, an infrastructure automation platform. 
        
The user said: "{request.message}"

Provide a helpful, conversational response. If they're asking about:
- System status or information: Provide what you can or explain how they can find it
- How something works: Give a clear explanation
- General questions: Answer helpfully and conversationally
- Requests for help: Offer guidance and suggestions

Be friendly, professional, and helpful. Don't mention that you're in "analysis mode" or anything technical about your processing."""

        response_result = await intent_brain_instance.llm_engine.generate(conversation_prompt)
        
        # Extract the response text
        if isinstance(response_result, dict):
            ai_response = response_result.get('generated_text', str(response_result))
        else:
            ai_response = str(response_result)
        
        # Build intent classification
        intent_classification = {
            "intent_type": "conversation",
            "confidence": 0.9,
            "method": "llm_conversation",
            "alternatives": [],
            "entities": [],
            "context_analysis": {
                "confidence_score": 0.9,
                "risk_level": "LOW",
                "requirements_count": 0,
                "recommendations": ["Conversational response provided"]
            },
            "reasoning": "User engaged in conversation",
            "metadata": {
                "engine": "llm_conversation_handler",
                "version": "2.0.0",
                "success": True,
                "conversation_type": intent_analysis.get("conversation_type", "general")
            }
        }
        
        return {
            "response": ai_response.strip(),
            "conversation_id": request.conversation_id,
            "intent": "conversation",
            "confidence": 0.9,
            "job_id": None,
            "execution_id": None,
            "automation_job_id": None,
            "workflow": None,
            "execution_started": False,
            "intent_classification": intent_classification,
            "timestamp": datetime.utcnow().isoformat(),
            "_routing": {
                "service_type": "llm_conversation", 
                "response_time": 0.0,
                "cached": False
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Conversation handling failed: {e}")
        return {
            "response": f"I encountered an issue during our conversation: {str(e)}. Please try again.",
            "conversation_id": request.conversation_id,
            "intent": "conversation_error",
            "confidence": 0.7,
            "job_id": None,
            "execution_id": None,
            "automation_job_id": None,
            "workflow": None,
            "execution_started": False,
            "intent_classification": {
                "intent_type": "conversation_error",
                "confidence": 0.7,
                "method": "pure_llm_conversation",
                "alternatives": [],
                "entities": [],
                "context_analysis": {"error": str(e)},
                "reasoning": "Exception in conversation handling",
                "metadata": {
                    "engine": "llm_conversation_handler",
                    "success": False,
                    "error": str(e)
                }
            },
            "timestamp": datetime.utcnow().isoformat(),
            "_routing": {
                "service_type": "pure_llm_conversation_exception", 
                "response_time": 0.0,
                "cached": False
            }
        }

class JobRequest(BaseModel):
    description: str
    user_id: Optional[int] = None
    priority: str = "normal"

class JobResponse(BaseModel):
    job_id: str
    workflow: Dict[str, Any]
    message: str
    confidence: float
    parsed_request: Dict[str, Any]
    # Phase 7 Integration Fields
    targets: Optional[List[Dict[str, Any]]] = []
    execution_plan: Optional[Dict[str, Any]] = {}
    optimizations: Optional[List[Dict[str, Any]]] = []

class TextAnalysisRequest(BaseModel):
    text: str

class TextAnalysisResponse(BaseModel):
    text: str
    parsed_request: Dict[str, Any]
    entities: Dict[str, List[str]]
    confidence: float
    suggestions: List[str]

class ExecuteJobRequest(BaseModel):
    description: str
    execute_immediately: bool = True
    user_id: Optional[int] = None

class ExecuteJobResponse(BaseModel):
    job_id: str
    execution_id: Optional[str] = None
    task_id: Optional[str] = None
    workflow: Dict[str, Any]
    message: str
    confidence: float
    execution_started: bool
    automation_job_id: Optional[int] = None

class ChatRequest(BaseModel):
    message: str
    user_id: Optional[int] = None
    conversation_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    intent: str  # "question", "job_creation", "unknown"
    confidence: float
    conversation_id: Optional[str] = None
    job_id: Optional[str] = None
    execution_id: Optional[str] = None
    automation_job_id: Optional[int] = None
    workflow: Optional[Dict[str, Any]] = None
    execution_started: bool = False
    # Enhanced clarification and risk assessment fields
    clarification_needed: Optional[bool] = False
    clarifying_questions: Optional[List[str]] = None
    missing_information: Optional[List[str]] = None
    risk_assessment: Optional[Dict[str, Any]] = None
    field_confidence_scores: Optional[Dict[str, Dict[str, Any]]] = None
    validation_issues: Optional[List[Dict[str, Any]]] = None
    # Debug information
    intent_classification: Optional[Dict[str, Any]] = None
    timestamp: Optional[str] = None
    _routing: Optional[Dict[str, Any]] = None
    # Real-time thinking visualization
    thinking_session_id: Optional[str] = None

@app.get("/health")
async def health_check():
    """Health check endpoint with pure LLM architecture status"""
    try:
        # Get detailed health status from Intent Brain
        brain_health = {"status": "healthy", "brain_type": "intent_brain"}
        
        return {
            "status": "healthy",
            "service": "ai-service",
            "architecture": "pure_llm",
            "brain_engine": brain_health,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "degraded",
            "service": "ai-service",
            "architecture": "pure_llm",
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

@app.get("/info")
async def service_info():
    """Service information endpoint - Pure LLM Architecture"""
    return {
        "service": "ai-service",
        "version": "3.0.0",
        "architecture": "pure_llm",
        "description": "Pure LLM-powered automation service - NLM completely eliminated",
        "capabilities": [
            "Pure LLM natural language understanding",
            "Multi-stage LLM job creation pipeline",
            "LLM-powered conversation handling",
            "Intelligent intent routing via LLM",
            "Vector-powered knowledge base",
            "Multi-protocol support (SNMP, SMTP, SSH, VAPIX)",
            "Script generation (PowerShell, Bash, Python)",
            "Real-time system queries",
            "Continuous learning from interactions"
        ],
        "llm_features": [
            "Ollama LLM integration",
            "Multi-stage job creation (ANALYZE â†’ PLAN â†’ VALIDATE â†’ CREATE)",
            "Intelligent conversation routing",
            "Context-aware responses",
            "Safety validation and risk assessment",
            "Fallback error handling"
        ],
        "supported_protocols": [
            "SNMP - Network device monitoring",
            "SMTP - Email notifications and alerts", 
            "SSH - Remote command execution",
            "VAPIX - Axis camera integration"
        ],
        "supported_operations": [
            "Automation job creation", "System maintenance", "Deployment requests",
            "Configuration changes", "Monitoring setup", "Troubleshooting",
            "Network monitoring", "Email alerts", "Remote execution", 
            "Camera management", "Script generation", "System queries"
        ],
        "architecture_changes": {
            "nlm_status": "completely_removed",
            "intent_engine": "disabled",
            "job_creation": "pure_llm_pipeline",
            "conversation": "pure_llm_handler",
            "routing": "llm_based_intelligent_routing"
        },
        "automation_integration": True,
        "nlm_eliminated": True
    }

@app.get("/system-capabilities")
async def get_system_capabilities():
    """Get comprehensive system capabilities and self-awareness information"""
    try:
        # System capabilities not available in simplified intent brain architecture
        return {
            "status": "unavailable",
            "message": "System capabilities not available in simplified intent brain architecture",
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error("Failed to get system capabilities", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to get system capabilities: {str(e)}")

@app.post("/validate-job")
async def validate_job_request(request: dict):
    """Validate job request before creation"""
    try:
        description = request.get("description", "")
        user_id = request.get("user_id", "system")
        
        if not description:
            raise HTTPException(status_code=400, detail="Description is required")
        
        # Parse the request using NLP
        parsed_request = await intent_processor.parse_request(description)
        
        # Extract requirements
        requirements = {
            "description": description,
            "operation": parsed_request.operation,
            "target_process": parsed_request.target_process,
            "target_service": parsed_request.target_service,
            "target_group": parsed_request.target_group,
            "target_os": parsed_request.target_os
        }
        
        # Get target systems
        target_systems = []
        if parsed_request.target_group:
            targets = await asset_client.resolve_target_group(parsed_request.target_group)
            target_systems = [t.get('hostname', t.get('name', str(t))) for t in targets] if targets else []
        
        # Validate using job validator
        try:
            from job_engine.job_validator import JobValidator
            validator = JobValidator()
            
            validation_result = validator.validate_job_request(description)
        except ImportError as ie:
            logger.warning(f"Job validator not available: {ie}")
            # Return basic validation result
            return {
                "is_valid": True,
                "confidence_score": 0.7,
                "issues": [],
                "missing_requirements": [],
                "clarification_questions": [],
                "parsed_request": {
                    "operation": parsed_request.operation,
                    "target_process": parsed_request.target_process,
                    "target_service": parsed_request.target_service,
                    "target_group": parsed_request.target_group,
                    "target_os": parsed_request.target_os,
                    "confidence": parsed_request.confidence
                }
            }
        
        return {
            "is_valid": validation_result.is_valid,
            "confidence_score": validation_result.confidence_score,
            "issues": [],
            "missing_requirements": [],
            "clarification_questions": validation_result.clarification_questions,
            "risk_assessment": validation_result.risk_assessment,
            "field_confidence_scores": validation_result.field_confidence_scores,
            "parsed_request": {
                "operation": parsed_request.operation,
                "target_process": parsed_request.target_process,
                "target_service": parsed_request.target_service,
                "target_group": parsed_request.target_group,
                "target_os": parsed_request.target_os,
                "confidence": parsed_request.confidence
            }
        }
        
    except Exception as e:
        logger.error(f"Job validation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Validation failed: {str(e)}")

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """PURE LLM CHAT INTERFACE - NO MORE NLP PATTERN MATCHING BULLSHIT!"""
    result = await pure_llm_chat_endpoint(request, intent_brain)
    
    # Ensure the result matches ChatResponse format
    if isinstance(result, dict):
        # Map the result to ChatResponse fields
        chat_response = {
            "response": result.get("response", result.get("message", "No response generated")),
            "intent": result.get("intent", result.get("ai_brain_decision", "unknown")),
            "confidence": result.get("confidence", 0.8),
            "conversation_id": result.get("conversation_id", request.conversation_id),
            "job_id": result.get("job_id"),
            "execution_id": result.get("execution_id"),
            "automation_job_id": result.get("automation_job_id"),
            "workflow": result.get("workflow"),
            "execution_started": result.get("execution_started", False),
            "clarification_needed": result.get("clarification_needed", False),
            "clarifying_questions": result.get("clarifying_questions"),
            "missing_information": result.get("missing_information"),
            "risk_assessment": result.get("risk_assessment"),
            "field_confidence_scores": result.get("field_confidence_scores"),
            "validation_issues": result.get("validation_issues"),
            "intent_classification": result.get("intent_classification"),
            "timestamp": datetime.now().isoformat(),
            "_routing": result.get("_routing")
        }
        
        # Add thinking session ID if available
        if "thinking_session_id" in result:
            chat_response["thinking_session_id"] = result["thinking_session_id"]
        
        return ChatResponse(**chat_response)
    else:
        # Fallback for non-dict responses
        return ChatResponse(
            response=str(result),
            intent="unknown",
            confidence=0.5,
            conversation_id=request.conversation_id,
            timestamp=datetime.now().isoformat()
        )

@app.get("/fulfillment/status/{request_id}")
async def get_fulfillment_status(request_id: str):
    """Get the status of a fulfillment request"""
    try:
        logger.info(f"ðŸ” Getting fulfillment status for request: {request_id}")
        
        # Get status from the fulfillment engine
        status_info = await fulfillment_engine.get_fulfillment_status(request_id)
        
        if not status_info:
            raise HTTPException(status_code=404, detail=f"Fulfillment request {request_id} not found")
        
        return status_info.to_dict()
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get fulfillment status for {request_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get status: {str(e)}")

@app.get("/fulfillment/requests")
async def list_fulfillment_requests(limit: int = 50, offset: int = 0):
    """List recent fulfillment requests"""
    try:
        logger.info(f"ðŸ“‹ Listing fulfillment requests (limit: {limit}, offset: {offset})")
        
        # Get request list from the fulfillment engine
        requests = await fulfillment_engine.list_active_fulfillments()
        
        # Apply limit and offset manually since the engine doesn't support it
        total_requests = len(requests)
        paginated_requests = requests[offset:offset + limit] if requests else []
        
        return {
            "requests": [req.to_dict() for req in paginated_requests],
            "total": total_requests,
            "limit": limit,
            "offset": offset
        }
        
    except Exception as e:
        logger.error(f"Failed to list fulfillment requests: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list requests: {str(e)}")

class ProceedWithRiskRequest(BaseModel):
    message: str
    conversation_id: str
    acknowledge_risks: bool = False
    user_notes: Optional[str] = None

@app.post("/proceed-with-risk")
async def proceed_with_risk(request: ProceedWithRiskRequest):
    """Allow user to proceed with job creation despite risks after acknowledging them"""
    try:
        if not request.acknowledge_risks:
            raise HTTPException(status_code=400, detail="Must acknowledge risks to proceed")
        
        # Log the risk acknowledgment
        logger.warning(
            "User proceeding with risks acknowledged",
            conversation_id=request.conversation_id,
            user_notes=request.user_notes,
            message=request.message
        )
        
        # Process through intent brain with risk acknowledgment
        response = await intent_brain.analyze_intent(
            message=request.message,
            user_context={"user_id": "system"}
        )
        
        return {
            "message": "Job created with acknowledged risks",
            "job_id": str(response.get("job_id")) if response.get("job_id") is not None else None,
            "execution_id": response.get("execution_id"),
            "automation_job_id": response.get("automation_job_id"),
            "workflow": response.get("workflow"),
            "execution_started": response.get("execution_started", False),
            "risk_acknowledged": True,
            "user_notes": request.user_notes
        }
        
    except Exception as e:
        logger.error(f"Failed to proceed with risk: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Risk assessment is now included in the chat response

# System queries and script generation are now handled through the main chat interface

@app.get("/knowledge-stats")
def get_knowledge_stats():
    """Get AI knowledge base statistics"""
    try:
        if not intent_brain:
            raise HTTPException(status_code=503, detail="Intent Brain not initialized")
        
        # Knowledge stats not available in simplified intent brain architecture
        stats = {"message": "Knowledge stats not available in simplified architecture"}
        return {
            "status": "success",
            "stats": stats,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error("Failed to get knowledge stats", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to get stats: {str(e)}")

@app.post("/store-knowledge")
async def store_knowledge_endpoint(request: dict):
    """Store new knowledge in the AI system"""
    try:
        content = request.get("content", "")
        category = request.get("category", "general")
        
        if not content:
            raise HTTPException(status_code=400, detail="Content is required")
        
        # Knowledge storage not available in simplified intent brain architecture
        success = False
        
        if success:
            return {
                "status": "success",
                "message": f"Knowledge stored successfully in category '{category}'"
            }
        else:
            raise HTTPException(status_code=503, detail="Failed to store knowledge")
            
    except Exception as e:
        logger.error("Failed to store knowledge", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to store knowledge: {str(e)}")

@app.post("/protocol/execute")
async def execute_protocol_operation(request: dict):
    """Execute operation using specified protocol"""
    try:
        protocol = request.get("protocol", "")
        target = request.get("target", {})
        command = request.get("command", "")
        credentials = request.get("credentials", {})
        kwargs = request.get("parameters", {})
        
        if not protocol or not target or not command:
            raise HTTPException(status_code=400, detail="Protocol, target, and command are required")
        
        logger.info("Executing protocol operation", 
                   protocol=protocol, 
                   target=target.get("hostname", "unknown"),
                   command=command)
        
        # Protocol execution not available in simplified intent brain architecture
        result = {"error": "Protocol execution not available in simplified architecture"}
        
        return result
        
    except Exception as e:
        logger.error("Protocol operation failed", error=str(e))
        raise HTTPException(status_code=500, detail=f"Protocol operation failed: {str(e)}")

# Protocol-specific operations are now handled through the main chat interface and /protocol/execute endpoint

@app.get("/protocols/capabilities")
async def get_protocol_capabilities():
    """Get all supported protocol capabilities"""
    try:
        # Protocol status not available in simplified intent brain architecture
        status = {"error": "Protocol status not available in simplified architecture"}
        return status
        
    except Exception as e:
        logger.error("Failed to get protocol capabilities", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to get capabilities: {str(e)}")

def _map_operation_to_intent(operation: str) -> str:
    """Map operation to intent type for workflow generation"""
    operation_to_intent_map = {
        "update": "system_maintenance",
        "restart": "system_maintenance", 
        "stop": "system_maintenance",
        "start": "system_maintenance",
        "install": "deployment_request",
        "remove": "system_maintenance",
        "check": "information_query",
        "status": "information_query",
        "deploy": "deployment_request",
        "configure": "configuration_change",
        "backup": "backup_request",
        "restore": "backup_request",
        "monitor": "monitoring_setup",
        "audit": "security_audit",
        "troubleshoot": "troubleshooting"
    }
    return operation_to_intent_map.get(operation, "system_maintenance")

@app.post("/create-job", response_model=JobResponse)
async def create_job(request: JobRequest):
    """Create a new automation job from natural language description"""
    try:
        logger.info("Processing job creation request", description=request.description)
        
        # Step 1: Parse the natural language request
        parsed_request = await intent_processor.parse_request(request.description)
        logger.info("Parsed request", 
                   operation=parsed_request.operation,
                   target_process=parsed_request.target_process,
                   target_group=parsed_request.target_group,
                   confidence=parsed_request.confidence)
        
        # Step 2: Resolve target groups to actual targets
        target_groups = []
        if parsed_request.target_group:
            targets = await asset_client.resolve_target_group(parsed_request.target_group)
            if not targets:
                # Use mock data for demo
                targets = await asset_client.create_mock_targets_for_demo(parsed_request.target_group)
            
            if targets:
                target_groups = [parsed_request.target_group]
                logger.info(f"Resolved target group '{parsed_request.target_group}' to {len(targets)} targets")
            else:
                logger.warning(f"No targets found for group '{parsed_request.target_group}'")
        
        # Step 3: Phase 7 Integration - Use AI Brain Engine for complete workflow processing
        resolved_targets = []
        execution_plan = {}
        optimizations = []
        
        # Job creation through intent brain (simplified)
        if True:  # Always enabled in simplified architecture
            # Use Intent Brain for job analysis
            ai_response = {"workflow": {}, "targets": [], "execution_plan": {}, "optimizations": []}
            
            # Extract Phase 7 results from AI response
            workflow_dict = ai_response.get("workflow", {})
            resolved_targets = ai_response.get("targets", [])
            execution_plan = ai_response.get("execution_plan", {})
            optimizations = ai_response.get("optimizations", [])
            
            # If AI Brain didn't generate complete workflow, fall back to direct workflow generation
            if not workflow_dict.get("steps"):
                logger.info("AI Brain didn't generate complete workflow, using Phase 7 modules directly")
                
                # Convert parsed_request to the format expected by workflow_generator
                intent_type = _map_operation_to_intent(parsed_request.operation)
                requirements = {
                    "operation": parsed_request.operation,
                    "target_process": parsed_request.target_process,
                    "target_service": parsed_request.target_service,
                    "target_group": parsed_request.target_group,
                    "target_os": parsed_request.target_os,
                    "description": request.description
                }
                
                # Phase 7 Module Integration (simplified)
                # Step 3a: Target Resolution (not available in simplified architecture)
                resolved_targets = []
                
                # Step 3b: Workflow Generation (not available in simplified architecture)
                workflow = None
                
                # Step 3c: Step Optimization (not available in simplified architecture)
                optimized_workflow_obj = None
                optimizations = []
                
                # Step 3d: Execution Planning (not available in simplified architecture)
                execution_plan = {}
                
                # Convert GeneratedWorkflow to dict format for compatibility (simplified)
                workflow_dict = {
                    "id": "simplified_workflow",
                    "name": "Simplified Workflow",
                    "description": "Workflow generation not available in simplified architecture",
                    "type": "simplified",
                    "steps": []
                }
        else:
            # Fallback to basic workflow generation
            logger.warning("AI Brain job creation not enabled, using basic workflow generation")
            intent_type = _map_operation_to_intent(parsed_request.operation)
            requirements = {
                "operation": parsed_request.operation,
                "target_process": parsed_request.target_process,
                "target_service": parsed_request.target_service,
                "target_group": parsed_request.target_group,
                "target_os": parsed_request.target_os,
                "description": request.description
            }
            
            workflow = workflow_generator.generate_workflow(intent_type, requirements, target_groups)
            
            # Convert GeneratedWorkflow to dict format for compatibility
            workflow_dict = {
                "id": workflow.workflow_id,
                "name": workflow.name,
                "description": workflow.description,
                "type": workflow.workflow_type.value,
                "steps": [
                    {
                        "id": step.step_id,
                        "name": step.name,
                        "description": step.description,
                        "type": step.step_type.value,
                        "command": step.command,
                        "script": step.script,
                        "parameters": step.parameters,
                        "timeout": step.timeout,
                        "retry_count": step.retry_count,
                        "risk_level": step.risk_level,
                        "requires_approval": step.requires_approval
                    }
                    for step in workflow.steps
                ],
                "execution_mode": workflow.execution_mode.value,
                "estimated_duration": workflow.estimated_duration,
                "risk_level": workflow.risk_level,
                "requires_approval": workflow.requires_approval,
                "target_systems": workflow.target_systems
            }
        
        logger.info("Generated workflow", workflow_id=workflow_dict['id'], steps=len(workflow_dict['steps']))
        
        # Step 4: Create job ID
        job_id = str(uuid.uuid4())
        
        # Step 5: Prepare response
        message = f"Successfully created workflow for: {request.description}"
        if parsed_request.confidence < 0.5:
            message += f" (Low confidence: {parsed_request.confidence:.2f} - please verify)"
        
        return JobResponse(
            job_id=job_id,
            workflow=workflow_dict,
            message=message,
            confidence=parsed_request.confidence,
            parsed_request={
                "operation": parsed_request.operation,
                "target_process": parsed_request.target_process,
                "target_service": parsed_request.target_service,
                "target_group": parsed_request.target_group,
                "target_os": parsed_request.target_os,
                "raw_text": parsed_request.raw_text
            },
            # Phase 7 Integration Results
            targets=resolved_targets,
            execution_plan=execution_plan,
            optimizations=optimizations
        )
        
    except Exception as e:
        logger.error("Failed to create job", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to create job: {str(e)}")

@app.post("/analyze-text", response_model=TextAnalysisResponse)
async def analyze_text(request: TextAnalysisRequest):
    """Analyze text for automation intent"""
    try:
        logger.info("Analyzing text", text=request.text)
        
        # Parse the request
        parsed_request = await intent_processor.parse_request(request.text)
        
        # Extract all entities for debugging
        entities = await intent_processor.extract_entities(request.text)
        
        # Generate suggestions
        suggestions = []
        if parsed_request.confidence < 0.5:
            suggestions.append("Consider being more specific about the target group")
            suggestions.append("Specify the exact process or service name")
            
        if not parsed_request.target_group:
            suggestions.append("Add a target group (e.g., 'on CIS servers', 'on web servers')")
            
        if parsed_request.operation == "unknown":
            suggestions.append("Specify the operation (update, restart, stop, start, check)")
        
        return TextAnalysisResponse(
            text=request.text,
            parsed_request={
                "operation": parsed_request.operation,
                "target_process": parsed_request.target_process,
                "target_service": parsed_request.target_service,
                "target_group": parsed_request.target_group,
                "target_os": parsed_request.target_os,
                "confidence": parsed_request.confidence
            },
            entities=entities,
            confidence=parsed_request.confidence,
            suggestions=suggestions
        )
        
    except Exception as e:
        logger.error("Failed to analyze text", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to analyze text: {str(e)}")

@app.post("/execute-job", response_model=ExecuteJobResponse)
async def execute_job(request: ExecuteJobRequest):
    """Create and execute a job immediately"""
    try:
        logger.info("Processing job execution request", 
                   description=request.description,
                   execute_immediately=request.execute_immediately)
        
        # Step 1: Parse the natural language request
        parsed_request = await intent_processor.parse_request(request.description)
        logger.info("Parsed execution request", 
                   operation=parsed_request.operation,
                   target_process=parsed_request.target_process,
                   target_group=parsed_request.target_group,
                   confidence=parsed_request.confidence)
        
        # Step 2: Resolve target groups to actual targets
        target_groups = []
        if parsed_request.target_group:
            targets = await asset_client.resolve_target_group(parsed_request.target_group)
            if not targets:
                # Use mock data for demo
                targets = await asset_client.create_mock_targets_for_demo(parsed_request.target_group)
            
            if targets:
                target_groups = [parsed_request.target_group]
                logger.info(f"Resolved target group '{parsed_request.target_group}' to {len(targets)} targets")
            else:
                logger.warning(f"No targets found for group '{parsed_request.target_group}'")
        
        # Step 3: Generate workflow
        # Convert parsed_request to the format expected by workflow_generator
        intent_type = _map_operation_to_intent(parsed_request.operation)
        requirements = {
            "operation": parsed_request.operation,
            "target_process": parsed_request.target_process,
            "target_service": parsed_request.target_service,
            "target_group": parsed_request.target_group,
            "target_os": parsed_request.target_os,
            "description": request.description
        }
        
        workflow = workflow_generator.generate_workflow(intent_type, requirements, target_groups)
        
        # Convert GeneratedWorkflow to dict format for compatibility
        workflow_dict = {
            "id": workflow.workflow_id,
            "name": workflow.name,
            "description": workflow.description,
            "type": workflow.workflow_type.value,
            "steps": [
                {
                    "id": step.step_id,
                    "name": step.name,
                    "description": step.description,
                    "type": step.step_type.value,
                    "command": step.command,
                    "script": step.script,
                    "parameters": step.parameters,
                    "timeout": step.timeout,
                    "retry_count": step.retry_count,
                    "risk_level": step.risk_level,
                    "requires_approval": step.requires_approval
                }
                for step in workflow.steps
            ],
            "execution_mode": workflow.execution_mode.value,
            "estimated_duration": workflow.estimated_duration,
            "risk_level": workflow.risk_level,
            "requires_approval": workflow.requires_approval,
            "target_systems": workflow.target_systems
        }
        
        logger.info("Generated workflow for execution", 
                   workflow_id=workflow_dict['id'], 
                   steps=len(workflow_dict['steps']))
        
        # Step 4: Create job ID
        job_id = str(uuid.uuid4())
        
        # Step 5: Submit to automation service if requested
        execution_started = False
        automation_job_id = None
        execution_id = None
        task_id = None
        
        if request.execute_immediately:
            try:
                # Check automation service health
                automation_healthy = await automation_client.health_check()
                if not automation_healthy:
                    logger.warning("Automation service not healthy, skipping execution")
                    message = f"Workflow created but automation service unavailable for execution"
                else:
                    # Submit workflow to automation service
                    submission_result = await automation_client.submit_ai_workflow(
                        workflow_dict, 
                        job_name=f"AI: {request.description[:50]}..."
                    )
                    
                    if submission_result.get('success'):
                        execution_started = True
                        automation_job_id = submission_result.get('job_id')
                        execution_id = submission_result.get('execution_id')
                        task_id = submission_result.get('task_id')
                        
                        logger.info("Workflow submitted to automation service",
                                   automation_job_id=automation_job_id,
                                   execution_id=execution_id)
                        
                        message = f"Workflow created and execution started (Job ID: {automation_job_id})"
                    else:
                        logger.error("Failed to submit workflow to automation service",
                                    error=submission_result.get('error'))
                        message = f"Workflow created but execution failed: {submission_result.get('error')}"
                        
            except Exception as e:
                logger.error("Failed to submit workflow for execution", error=str(e))
                message = f"Workflow created but execution failed: {str(e)}"
        else:
            message = f"Workflow created successfully (execution not requested)"
        
        # Add confidence warning if needed
        if parsed_request.confidence < 0.5:
            message += f" (Low confidence: {parsed_request.confidence:.2f} - please verify)"
        
        return ExecuteJobResponse(
            job_id=job_id,
            execution_id=execution_id,
            task_id=task_id,
            workflow=workflow_dict,
            message=message,
            confidence=parsed_request.confidence,
            execution_started=execution_started,
            automation_job_id=automation_job_id
        )
        
    except Exception as e:
        logger.error("Failed to execute job", error=str(e))
        raise HTTPException(status_code=500, detail=f"Failed to execute job: {str(e)}")

@app.get("/test-nlp")
async def test_nlp():
    """Test endpoint for NLP functionality"""
    test_requests = [
        "update stationcontroller on CIS servers",
        "restart nginx on web servers",
        "stop Apache service on all servers",
        "check status of IIS on production servers"
    ]
    
    results = []
    for test_text in test_requests:
        parsed = await intent_processor.parse_request(test_text)
        entities = await intent_processor.extract_entities(test_text)
        
        results.append({
            "input": test_text,
            "parsed": {
                "operation": parsed.operation,
                "target_process": parsed.target_process,
                "target_service": parsed.target_service,
                "target_group": parsed.target_group,
                "target_os": parsed.target_os,
                "confidence": parsed.confidence
            },
            "entities": entities
        })
    
    return {"test_results": results}

@app.get("/test-workflow")
async def test_workflow():
    """Test endpoint for workflow generation"""
    test_request = "update stationcontroller on CIS servers"
    parsed = await intent_processor.parse_request(test_request)
    
    # Convert to proper format for workflow generator
    intent_type = _map_operation_to_intent(parsed.operation)
    requirements = {
        "operation": parsed.operation,
        "target_process": parsed.target_process,
        "target_service": parsed.target_service,
        "target_group": parsed.target_group,
        "target_os": parsed.target_os,
        "description": test_request
    }
    
    workflow = workflow_generator.generate_workflow(intent_type, requirements, ["CIS"])
    
    # Convert to dict format
    workflow_dict = {
        "id": workflow.workflow_id,
        "name": workflow.name,
        "description": workflow.description,
        "type": workflow.workflow_type.value,
        "steps": [
            {
                "id": step.step_id,
                "name": step.name,
                "description": step.description,
                "type": step.step_type.value,
                "command": step.command,
                "script": step.script,
                "parameters": step.parameters,
                "timeout": step.timeout,
                "retry_count": step.retry_count,
                "risk_level": step.risk_level,
                "requires_approval": step.requires_approval
            }
            for step in workflow.steps
        ],
        "execution_mode": workflow.execution_mode.value,
        "estimated_duration": workflow.estimated_duration,
        "risk_level": workflow.risk_level,
        "requires_approval": workflow.requires_approval,
        "target_systems": workflow.target_systems
    }
    
    return {
        "input": test_request,
        "parsed": {
            "operation": parsed.operation,
            "target_process": parsed.target_process,
            "target_group": parsed.target_group,
            "confidence": parsed.confidence
        },
        "workflow": workflow_dict
    }

@app.get("/test-assets")
async def test_assets():
    """Test endpoint for asset service integration"""
    try:
        # Test health check
        healthy = await asset_client.health_check()
        
        # Test group resolution
        test_groups = ["CIS servers", "web servers"]
        group_results = {}
        
        for group_name in test_groups:
            targets = await asset_client.resolve_target_group(group_name)
            if not targets:
                targets = await asset_client.create_mock_targets_for_demo(group_name)
            group_results[group_name] = {
                "target_count": len(targets),
                "targets": [{"hostname": t.get("hostname"), "os_type": t.get("os_type")} for t in targets]
            }
        
        return {
            "asset_service_healthy": healthy,
            "group_resolution": group_results
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "asset_service_healthy": False
        }

@app.get("/test-automation")
async def test_automation():
    """Test endpoint for automation service integration"""
    try:
        # Test health check
        healthy = await automation_client.health_check()
        
        # Get client info
        client_info = automation_client.get_client_info()
        
        # List AI jobs
        ai_jobs = await automation_client.list_ai_jobs(limit=5)
        
        return {
            "automation_service_healthy": healthy,
            "client_info": client_info,
            "ai_jobs_count": len(ai_jobs),
            "recent_ai_jobs": [
                {
                    "id": job.get("id"),
                    "name": job.get("name"),
                    "created_at": job.get("created_at"),
                    "job_type": job.get("job_type")
                } for job in ai_jobs[:3]
            ]
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "automation_service_healthy": False
        }

@app.get("/test-integration")
async def test_integration():
    """Test full AI to Automation integration"""
    try:
        test_request = "check status of nginx on web servers"
        
        logger.info("Testing full integration", request=test_request)
        
        # Step 1: Test AI processing
        parsed = await intent_processor.parse_request(test_request)
        
        # Convert to proper format for workflow generator
        intent_type = _map_operation_to_intent(parsed.operation)
        requirements = {
            "operation": parsed.operation,
            "target_process": parsed.target_process,
            "target_service": parsed.target_service,
            "target_group": parsed.target_group,
            "target_os": parsed.target_os,
            "description": test_request
        }
        
        workflow = workflow_generator.generate_workflow(intent_type, requirements, ["web servers"])
        
        # Convert to dict format
        workflow_dict = {
            "id": workflow.workflow_id,
            "name": workflow.name,
            "description": workflow.description,
            "type": workflow.workflow_type.value,
            "steps": [
                {
                    "id": step.step_id,
                    "name": step.name,
                    "description": step.description,
                    "type": step.step_type.value,
                    "command": step.command,
                    "script": step.script,
                    "parameters": step.parameters,
                    "timeout": step.timeout,
                    "retry_count": step.retry_count,
                    "risk_level": step.risk_level,
                    "requires_approval": step.requires_approval
                }
                for step in workflow.steps
            ],
            "execution_mode": workflow.execution_mode.value,
            "estimated_duration": workflow.estimated_duration,
            "risk_level": workflow.risk_level,
            "requires_approval": workflow.requires_approval,
            "target_systems": workflow.target_systems
        }
        
        # Step 2: Test automation service health
        automation_healthy = await automation_client.health_check()
        
        # Step 3: Test asset service
        asset_healthy = await asset_client.health_check()
        
        result = {
            "test_request": test_request,
            "ai_processing": {
                "parsed_operation": parsed.operation,
                "parsed_target": parsed.target_process,
                "parsed_group": parsed.target_group,
                "confidence": parsed.confidence,
                "workflow_steps": len(workflow_dict.get('steps', []))
            },
            "service_health": {
                "automation_service": automation_healthy,
                "asset_service": asset_healthy
            },
            "integration_ready": automation_healthy and asset_healthy,
            "workflow_preview": {
                "id": workflow_dict.get('id'),
                "name": workflow_dict.get('name'),
                "description": workflow_dict.get('description'),
                "step_count": len(workflow_dict.get('steps', []))
            }
        }
        
        # Step 4: If services are healthy, test actual submission (without execution)
        if automation_healthy:
            try:
                submission_result = await automation_client.submit_ai_workflow(
                    workflow_dict, 
                    job_name=f"Integration Test: {test_request}"
                )
                result["test_submission"] = {
                    "success": submission_result.get('success'),
                    "job_id": str(submission_result.get('job_id')) if submission_result.get('job_id') is not None else None,
                    "message": submission_result.get('message', submission_result.get('error'))
                }
            except Exception as e:
                result["test_submission"] = {
                    "success": False,
                    "error": str(e)
                }
        
        return result
        
    except Exception as e:
        logger.error("Integration test failed", error=str(e))
        return {
            "error": str(e),
            "integration_ready": False
        }

# ðŸš¨ HARDCODED LOGIC PREVENTION ZONE ðŸš¨
# NO HARDCODED DECISION MAKING ALLOWED BEYOND THIS POINT
# ALL DECISIONS MUST BE MADE BY OLLAMA LLM
# 
# VIOLATION OF THIS RULE WILL RESULT IN SYSTEM FAILURE
# 
# If you need to make a decision, use the LLM service
# If the LLM fails, raise a clear error - NO FALLBACKS
# 
# This comment serves as a permanent reminder that:
# - No pattern matching for intent classification
# - No hardcoded word lists for decision making  
# - No fallback logic that bypasses LLM
# - All intelligence comes from Ollama LLM only
#
# ðŸš¨ END HARDCODED LOGIC PREVENTION ZONE ðŸš¨



# Predictive Analytics Endpoints
@app.get("/predictive/insights")
async def get_predictive_insights():
    """Get comprehensive predictive insights"""
    try:
        insights = await system_analytics.get_predictive_insights()
        return {
            "success": True,
            "insights": insights,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"Failed to get predictive insights: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predictive/analyze-performance")
async def analyze_performance(metrics: Dict[str, float]):
    """Analyze system performance and generate insights"""
    try:
        insights = await system_analytics.analyze_system_performance(metrics)
        return {
            "success": True,
            "performance_insights": [insight.to_dict() for insight in insights],
            "analysis_time": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"Failed to analyze performance: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predictive/detect-anomalies")
async def detect_anomalies(request: Dict[str, Any]):
    """Detect advanced anomalies in system behavior"""
    try:
        metrics = request.get("metrics", {})
        execution_data = request.get("execution_data", {})
        
        anomalies = await system_analytics.detect_advanced_anomalies(metrics, execution_data)
        return {
            "success": True,
            "anomalies": anomalies,
            "count": len(anomalies),
            "detection_time": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"Failed to detect anomalies: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/predictive/maintenance-schedule")
async def get_maintenance_schedule():
    """Get predictive maintenance recommendations"""
    try:
        # Get targets from asset service (simplified for demo)
        targets = [
            {"hostname": "server-01", "type": "server", "last_maintenance": "2024-01-01T00:00:00"},
            {"hostname": "server-02", "type": "server", "last_maintenance": "2024-02-01T00:00:00"},
            {"hostname": "switch-01", "type": "network_device", "last_maintenance": "2024-03-01T00:00:00"}
        ]
        
        recommendations = await system_analytics.generate_maintenance_schedule(targets)
        return {
            "success": True,
            "maintenance_recommendations": [rec.to_dict() for rec in recommendations],
            "count": len(recommendations),
            "generated_time": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"Failed to get maintenance schedule: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predictive/security-monitor")
async def monitor_security(log_entries: List[Dict[str, Any]]):
    """Monitor log entries for security events"""
    try:
        alerts = await system_analytics.monitor_security_events(log_entries)
        return {
            "success": True,
            "security_alerts": [alert.to_dict() for alert in alerts],
            "count": len(alerts),
            "analysis_time": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"Failed to monitor security: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status")
async def get_ai_status():
    """Get the status of the AI Brain Engine components"""
    try:
        # Check which components are available
        components_status = {
            "modern_components": {
                "system_analytics": False,
                "intent_processor": True,
                "system_capabilities": False,
                "ai_engine": False
            },
            "migration_complete": True,
            "active_mode": "modern",
            "version": "2.0.0",
            "features": {
                "llm_powered": True,
                "vector_storage": True,
                "intelligent_analysis": True,
                "natural_language": True
            }
        }
        
        return {
            "success": True,
            "ai_status": components_status,
            "message": "AI Brain is running with modern components"
        }
    except Exception as e:
        logger.error(f"Failed to get AI status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# API v1 routes for dashboard compatibility
@app.get("/api/v1/health")
async def api_v1_health_check():
    """API v1 compatible health check endpoint for dashboard"""
    try:
        # Get detailed health status from Intent Brain
        brain_health = {"status": "healthy", "brain_type": "intent_brain"}
        
        # Transform to expected dashboard format
        services = {
            "ai-brain": {
                "status": "healthy" if brain_health.get("status") == "healthy" else "unhealthy",
                "service": "ai-brain",
                "components": brain_health.get("components", {}),
                "architecture": brain_health.get("architecture", "pure_llm"),
                "timestamp": brain_health.get("timestamp")
            }
        }
        
        return {
            "status": "healthy",
            "services": services,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"API v1 health check failed: {e}")
        return {
            "status": "unhealthy",
            "services": {
                "ai-brain": {
                    "status": "unhealthy",
                    "service": "ai-brain",
                    "error": str(e),
                    "timestamp": datetime.utcnow().isoformat()
                }
            },
            "timestamp": datetime.utcnow().isoformat()
        }

@app.get("/api/v1/monitoring/dashboard")
async def api_v1_monitoring_dashboard():
    """API v1 compatible monitoring dashboard endpoint"""
    try:
        # Get current health status
        brain_health = {"status": "healthy", "brain_type": "intent_brain"}
        
        # Create dashboard data structure
        current_services = {
            "ai-brain": {
                "status": "healthy" if brain_health.get("status") == "healthy" else "unhealthy",
                "service": "ai-brain",
                "components": brain_health.get("components", {}),
                "architecture": brain_health.get("architecture", "pure_llm"),
                "response_time": 0.1,
                "timestamp": brain_health.get("timestamp")
            }
        }
        
        # Determine overall health
        overall_health = "healthy" if brain_health.get("status") == "healthy" else "degraded"
        
        # Create alerts based on component status
        alerts = []
        components = brain_health.get("components", {})
        for component, status in components.items():
            if not status:
                alerts.append({
                    "severity": "warning",
                    "service": "ai-brain",
                    "message": f"Component {component} is not available"
                })
        
        # Generate recommendations
        recommendations = []
        if alerts:
            recommendations.append("Some AI components are not fully initialized - this may affect functionality")
        else:
            recommendations.append("AI system is operating normally")
        
        return {
            "current": {
                "services": current_services,
                "overall_health": overall_health
            },
            "history": [],  # Could be populated with historical data
            "analysis": {
                "overall_health": overall_health,
                "alerts": alerts,
                "recommendations": recommendations
            },
            "statistics": {
                "total_services": 1,
                "healthy_services": 1 if overall_health == "healthy" else 0,
                "unhealthy_services": 0 if overall_health == "healthy" else 1,
                "uptime_percentage": 100.0 if overall_health == "healthy" else 75.0
            }
        }
    except Exception as e:
        logger.error(f"API v1 monitoring dashboard failed: {e}")
        return {
            "current": {
                "services": {
                    "ai-brain": {
                        "status": "unhealthy",
                        "service": "ai-brain",
                        "error": str(e),
                        "timestamp": datetime.utcnow().isoformat()
                    }
                },
                "overall_health": "unhealthy"
            },
            "history": [],
            "analysis": {
                "overall_health": "unhealthy",
                "alerts": [{
                    "severity": "critical",
                    "service": "ai-brain",
                    "message": f"AI service error: {str(e)}"
                }],
                "recommendations": ["Check AI service logs and restart if necessary"]
            },
            "statistics": {
                "total_services": 1,
                "healthy_services": 0,
                "unhealthy_services": 1,
                "uptime_percentage": 0.0
            }
        }

@app.post("/api/v1/circuit-breaker/reset/{service_name}")
async def api_v1_reset_circuit_breaker(service_name: str):
    """API v1 compatible circuit breaker reset endpoint"""
    try:
        logger.info(f"Circuit breaker reset requested for service: {service_name}")
        
        # For AI brain, we can try to reinitialize components
        if service_name == "ai-brain":
            # Attempt to reinitialize the AI engine
            try:
                # This would reinitialize the AI engine if it has such capability
                # Reinitialize not available in simplified intent brain architecture
                return {"message": f"Circuit breaker reset acknowledged for {service_name} (no action needed)", "success": True}
            except Exception as reinit_error:
                logger.error(f"Failed to reinitialize {service_name}: {reinit_error}")
                return {"message": f"Circuit breaker reset attempted for {service_name} but reinitialize failed", "success": False}
        else:
            return {"message": f"Circuit breaker reset not applicable for {service_name}", "success": False}
            
    except Exception as e:
        logger.error(f"Circuit breaker reset failed for {service_name}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to reset circuit breaker: {str(e)}")

@app.get("/api/v1/knowledge-stats")
def api_v1_knowledge_stats():
    """API v1 compatible knowledge stats endpoint"""
    try:
        if not intent_brain:
            raise HTTPException(status_code=503, detail="Intent Brain not initialized")
        
        # Knowledge stats not available in simplified intent brain architecture
        stats = {"message": "Knowledge stats not available in simplified architecture"}
        return {
            "status": "success",
            "stats": stats,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error("Failed to get knowledge stats", error=str(e))
        # Return basic stats if detailed stats fail
        return {
            "status": "partial",
            "stats": {
                "total_documents": 0,
                "total_patterns": 0,
                "vector_store_status": "unavailable",
                "error": str(e)
            },
            "timestamp": datetime.utcnow().isoformat()
        }

@app.get("/knowledge-stats")
def knowledge_stats():
    """Knowledge stats endpoint for API gateway routing"""
    try:
        if not intent_brain:
            raise HTTPException(status_code=503, detail="Intent Brain not initialized")
        
        # Knowledge stats not available in simplified intent brain architecture
        stats = {"message": "Knowledge stats not available in simplified architecture"}
        return {
            "status": "success",
            "stats": stats,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error("Failed to get knowledge stats", error=str(e))
        # Return basic stats if detailed stats fail
        return {
            "status": "partial",
            "stats": {
                "total_documents": 0,
                "total_patterns": 0,
                "vector_store_status": "unavailable",
                "error": str(e)
            },
            "timestamp": datetime.utcnow().isoformat()
        }

# ============================================================================
# STRIPPED PATH ENDPOINTS FOR KONG GATEWAY
# ============================================================================

@app.get("/monitoring/dashboard")
async def monitoring_dashboard():
    """Monitoring dashboard endpoint for Kong gateway routing (stripped path)"""
    return await api_v1_monitoring_dashboard()

@app.post("/circuit-breaker/reset/{service_name}")
async def reset_circuit_breaker(service_name: str):
    """Circuit breaker reset endpoint for Kong gateway routing (stripped path)"""
    return await api_v1_reset_circuit_breaker(service_name)

@app.get("/knowledge-stats")
def knowledge_stats_stripped():
    """Knowledge stats endpoint for Kong gateway routing (stripped path)"""
    return api_v1_knowledge_stats()

# ============================================================================
# FULFILLMENT ENGINE API ENDPOINTS
# ============================================================================

@app.post("/fulfillment/start")
async def start_fulfillment(request: Dict[str, Any]):
    """Start fulfillment process"""
    try:
        logger.info("Starting fulfillment process via API")
        
        # Create AI understanding format from request
        ai_understanding = {
            "intent": request.get("intent", "job_request"),
            "response": request.get("description", ""),
            "original_message": request.get("message", ""),
            "conversation_id": request.get("conversation_id", f"api_{uuid.uuid4()}"),
            "intent_classification": {
                "intent_type": request.get("intent_type", "automation"),
                "confidence": request.get("confidence", 0.8),
                "method": "api_request",
                "alternatives": [],
                "entities": [],
                "context_analysis": {
                    "confidence_score": request.get("confidence", 0.8),
                    "risk_level": request.get("risk_level", "MEDIUM"),
                    "requirements_count": 1,
                    "recommendations": []
                },
                "reasoning": request.get("description", "API fulfillment request"),
                "metadata": {
                    "engine": "api_fulfillment",
                    "success": True
                }
            }
        }
        
        user_context = {
            "user_id": request.get("user_id", "api_user"),
            "conversation_id": request.get("conversation_id")
        }
        
        # Execute with Fulfillment Orchestrator
        fulfillment_result = await fulfillment_orchestrator.fulfill_intent(ai_understanding, user_context)
        
        return fulfillment_result.to_dict()
        
    except Exception as e:
        logger.error(f"Failed to start fulfillment: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to start fulfillment: {str(e)}")

@app.get("/fulfillment/{fulfillment_id}/status")
async def get_fulfillment_status(fulfillment_id: str):
    """Get fulfillment status"""
    try:
        result = await fulfillment_orchestrator.get_fulfillment_status(fulfillment_id)
        if not result:
            raise HTTPException(status_code=404, detail="Fulfillment not found")
        
        return result.to_dict()
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get fulfillment status: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get fulfillment status: {str(e)}")

@app.post("/fulfillment/{fulfillment_id}/cancel")
async def cancel_fulfillment(fulfillment_id: str):
    """Cancel fulfillment execution"""
    try:
        success = await fulfillment_orchestrator.cancel_fulfillment(fulfillment_id)
        if not success:
            raise HTTPException(status_code=404, detail="Fulfillment not found or cannot be cancelled")
        
        return {"success": True, "message": f"Fulfillment {fulfillment_id} cancelled"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to cancel fulfillment: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to cancel fulfillment: {str(e)}")

@app.get("/fulfillment/active")
async def list_active_fulfillments():
    """List active fulfillments"""
    try:
        active_fulfillments = await fulfillment_orchestrator.list_active_fulfillments()
        return {
            "active_fulfillments": [result.to_dict() for result in active_fulfillments],
            "count": len(active_fulfillments)
        }
        
    except Exception as e:
        logger.error(f"Failed to list active fulfillments: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to list active fulfillments: {str(e)}")

@app.get("/fulfillment/health")
async def get_fulfillment_health():
    """Get fulfillment orchestrator health"""
    try:
        health = await fulfillment_orchestrator.get_orchestrator_health()
        return health
        
    except Exception as e:
        logger.error(f"Failed to get fulfillment health: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get fulfillment health: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=3008)