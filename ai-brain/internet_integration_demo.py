#!/usr/bin/env python3
"""
Internet Data Integration Demo
Complete demonstration of feeding AI training system with internet data
"""

import json
import sqlite3
from datetime import datetime
from fixed_master_integration import FixedMasterIntegration

def demonstrate_internet_integration():
    """Complete demonstration of internet data integration"""
    
    print("ğŸŒ INTERNET DATA INTEGRATION FOR AI TRAINING SYSTEM")
    print("=" * 60)
    print("âœ… YES! You can absolutely feed your AI system with prompts from the Internet!")
    print("ğŸš€ We've built a comprehensive system that pulls from multiple sources:")
    print()
    
    # Show available sources
    print("ğŸ“¡ AVAILABLE INTERNET DATA SOURCES:")
    sources = [
        ("Stack Overflow", "Technical Q&A with voting scores", "âœ… Active"),
        ("GitHub Issues", "Real development problems and solutions", "âœ… Active"),
        ("Reddit", "Community discussions (r/devops, r/sysadmin)", "âœ… Active"),
        ("Hacker News", "Tech industry discussions", "âš ï¸  Limited"),
        ("Dev.to", "Developer articles and tutorials", "âœ… Active"),
        ("RSS Feeds", "Latest tech news and updates", "âœ… Active")
    ]
    
    for name, description, status in sources:
        print(f"  â€¢ {name:<15} - {description:<40} [{status}]")
    
    print()
    print("ğŸ¯ INTEGRATION CAPABILITIES:")
    capabilities = [
        "Real-time data fetching from multiple APIs",
        "Intelligent intent classification (6 categories)",
        "Quality scoring and filtering",
        "Complexity level assessment",
        "Rate limiting and error handling",
        "Unified database storage",
        "Continuous learning scheduling",
        "Export to training datasets"
    ]
    
    for cap in capabilities:
        print(f"  âœ… {cap}")
    
    print()
    
    # Run a small demonstration
    print("ğŸš€ RUNNING LIVE DEMONSTRATION...")
    print("-" * 40)
    
    integration = FixedMasterIntegration()
    
    # Small scale demo
    total_examples = integration.integrate_all_data(internet_count=50, generated_count=100)
    
    # Show statistics
    stats = integration.get_statistics()
    
    print()
    print("ğŸ“Š DEMONSTRATION RESULTS:")
    print(f"  ğŸŒ Internet Examples: {stats['source_distribution'].get('internet', 0)}")
    print(f"  ğŸ§  Generated Examples: {stats['source_distribution'].get('generated', 0)}")
    print(f"  ğŸ“ˆ Total Dataset Size: {stats['total_count']}")
    print(f"  â­ Average Quality: {stats['average_quality']:.2f}/1.0")
    print(f"  ğŸ¯ Average Confidence: {stats['average_confidence']:.1%}")
    
    # Export sample
    sample_path = "/tmp/internet_integration_sample.json"
    integration.export_master_dataset(sample_path)
    
    print(f"  ğŸ“ Sample Dataset: {sample_path}")
    
    return stats

def show_data_quality_analysis():
    """Analyze the quality of internet vs generated data"""
    
    print("\nğŸ” DATA QUALITY ANALYSIS:")
    print("-" * 30)
    
    with open('/tmp/fixed_master_dataset.json', 'r') as f:
        data = json.load(f)
    
    internet_data = [ex for ex in data if ex['source_type'] == 'internet']
    generated_data = [ex for ex in data if ex['source_type'] == 'generated']
    
    print(f"ğŸ“Š Internet Data Analysis ({len(internet_data)} examples):")
    if internet_data:
        avg_quality = sum(ex['quality_score'] for ex in internet_data) / len(internet_data)
        avg_confidence = sum(ex['confidence'] for ex in internet_data) / len(internet_data)
        print(f"  â€¢ Average Quality: {avg_quality:.2f}/1.0")
        print(f"  â€¢ Average Confidence: {avg_confidence:.1%}")
        
        # Source breakdown
        sources = {}
        for ex in internet_data:
            source = ex['source_name']
            sources[source] = sources.get(source, 0) + 1
        
        print("  â€¢ Source Distribution:")
        for source, count in sources.items():
            print(f"    - {source.title()}: {count} examples")
    
    print(f"\nğŸ§  Generated Data Analysis ({len(generated_data)} examples):")
    if generated_data:
        avg_quality = sum(ex['quality_score'] for ex in generated_data) / len(generated_data)
        avg_confidence = sum(ex['confidence'] for ex in generated_data) / len(generated_data)
        avg_complexity = sum(ex['complexity_score'] for ex in generated_data) / len(generated_data)
        print(f"  â€¢ Average Quality: {avg_quality:.2f}/1.0")
        print(f"  â€¢ Average Confidence: {avg_confidence:.1%}")
        print(f"  â€¢ Average Complexity: {avg_complexity:.1f}/5.0")

def show_continuous_learning_setup():
    """Show how to set up continuous learning"""
    
    print("\nğŸ”„ CONTINUOUS LEARNING SETUP:")
    print("-" * 35)
    print("For continuous internet data feeding, you can:")
    print()
    print("1ï¸âƒ£  Use the Continuous Internet Learner:")
    print("   python3 continuous_internet_learner.py")
    print("   â€¢ Runs every 30-120 minutes")
    print("   â€¢ Automatic quality filtering")
    print("   â€¢ Health monitoring")
    print()
    print("2ï¸âƒ£  Schedule with cron:")
    print("   # Every 2 hours")
    print("   0 */2 * * * cd /path/to/ai-brain && python3 fixed_master_integration.py")
    print()
    print("3ï¸âƒ£  Integrate with your existing pipeline:")
    print("   from fixed_master_integration import FixedMasterIntegration")
    print("   integration = FixedMasterIntegration()")
    print("   integration.integrate_all_data(internet_count=200, generated_count=500)")

def main():
    """Run the complete demonstration"""
    
    # Main demonstration
    stats = demonstrate_internet_integration()
    
    # Quality analysis
    show_data_quality_analysis()
    
    # Continuous learning setup
    show_continuous_learning_setup()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ CONCLUSION: INTERNET DATA INTEGRATION SUCCESS!")
    print("=" * 60)
    print("âœ… Your AI can now learn from real-world internet data")
    print("ğŸŒ Multiple high-quality sources integrated")
    print("ğŸ§  Combined with AI-generated examples for completeness")
    print("ğŸ“ˆ Continuous learning capability established")
    print("ğŸš€ Ready for production deployment!")
    print()
    print("ğŸ’¡ NEXT STEPS:")
    print("  1. Set up continuous learning schedule")
    print("  2. Monitor data quality metrics")
    print("  3. Expand to additional data sources")
    print("  4. Fine-tune intent classification")
    print("  5. Deploy to production AI system")

if __name__ == "__main__":
    main()