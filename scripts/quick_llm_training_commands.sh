#!/bin/bash
"""
Quick LLM Training Commands

This script provides ready-to-use commands for training your LLM
to fix the capability bypass issue and restore 90%+ accuracy.
"""

echo "üöÄ LLM Training Commands for Capability Fix"
echo "============================================"
echo ""

# Set up variables
TRAINING_DIR="/home/opsconductor/opsconductor-ng/training_data"
FORMATS_DIR="$TRAINING_DIR/formats"

echo "üìÅ Training Data Location: $TRAINING_DIR"
echo "üìä Available datasets:"
echo "   - training_data_1k.json (1,000 examples - quick testing)"
echo "   - training_data_5k.json (5,000 examples - recommended for fine-tuning)"
echo "   - training_data_10k.json (10,000 examples - comprehensive)"
echo "   - training_data_25k.json (25,000 examples - full coverage)"
echo ""

echo "üîß Training Formats Available:"
echo "   - $FORMATS_DIR/ollama_training.jsonl (Ollama format)"
echo "   - $FORMATS_DIR/huggingface_training.json (HuggingFace format)"
echo "   - $FORMATS_DIR/openai_training.jsonl (OpenAI format)"
echo ""

echo "üéØ OLLAMA FINE-TUNING COMMANDS:"
echo "==============================="
echo ""
echo "1. Create a Modelfile:"
cat << 'EOF'
# Save this as 'Modelfile' in your training directory
FROM llama2  # or your current model

# Set the training data
TEMPLATE """{{ .System }}
{{ .Prompt }}"""

# Optimize for JSON output
PARAMETER temperature 0.1
PARAMETER top_k 10
PARAMETER top_p 0.9
PARAMETER stop "</s>"
PARAMETER stop "Human:"
PARAMETER stop "Assistant:"
EOF
echo ""

echo "2. Train the model:"
echo "   cd $TRAINING_DIR"
echo "   ollama create opsconductor-intent-classifier -f Modelfile"
echo ""

echo "3. Test the trained model:"
echo "   ollama run opsconductor-intent-classifier"
echo "   >>> Classify: Display contents of /etc/hostname"
echo ""

echo "ü§ó HUGGINGFACE FINE-TUNING COMMANDS:"
echo "===================================="
echo ""
echo "1. Install dependencies:"
echo "   pip install transformers datasets torch accelerate"
echo ""

echo "2. Python training script:"
cat << 'EOF'
# Save as train_intent_classifier.py
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import Dataset
import json

# Load training data
with open("/home/opsconductor/opsconductor-ng/training_data/formats/huggingface_training.json", "r") as f:
    data = json.load(f)

# Create dataset
dataset = Dataset.from_list(data)

# Load model and tokenizer
model_name = "microsoft/DialoGPT-medium"  # or your preferred model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Set pad token
tokenizer.pad_token = tokenizer.eos_token

# Training arguments
training_args = TrainingArguments(
    output_dir="./intent-classifier-model",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=500,
    save_total_limit=2,
    prediction_loss_only=True,
    logging_steps=100,
)

# Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
)

# Train
trainer.train()
trainer.save_model()
EOF
echo ""

echo "3. Run training:"
echo "   python train_intent_classifier.py"
echo ""

echo "üî¨ TESTING YOUR TRAINED MODEL:"
echo "=============================="
echo ""
echo "Critical test cases to verify the fix:"
echo "1. 'Display contents of /etc/hostname' ‚Üí should return capabilities: ['system_info']"
echo "2. 'list all servers' ‚Üí should return capabilities: ['asset_management', 'resource_listing']"
echo "3. 'check disk space' ‚Üí should return capabilities: ['disk_monitoring', 'system_monitoring']"
echo ""

echo "Expected JSON format:"
cat << 'EOF'
{
  "category": "information",
  "action": "provide_information", 
  "confidence": 0.95,
  "capabilities": ["system_info"]
}
EOF
echo ""

echo "üö® VALIDATION CHECKLIST:"
echo "========================"
echo "‚úÖ LLM returns JSON with all required fields"
echo "‚úÖ 'capabilities' field is always present and non-empty"
echo "‚úÖ File reading requests map to 'system_info' capability"
echo "‚úÖ Monitoring requests include appropriate monitoring capabilities"
echo "‚úÖ Asset management requests include 'asset_management' capability"
echo ""

echo "üéØ SUCCESS CRITERIA:"
echo "==================="
echo "Once trained, your LLM should:"
echo "1. Always populate the 'capabilities' field"
echo "2. Map requests to appropriate capabilities"
echo "3. Trigger the HybridOrchestrator (no more bypasses!)"
echo "4. Achieve 90%+ accuracy through your sophisticated YAML tool selection"
echo ""

echo "üìû Need help? The training data is comprehensive and includes:"
echo "   - 25,000 diverse examples"
echo "   - All capability combinations"
echo "   - Proper category/action mappings"
echo "   - High-confidence training targets"
echo ""

echo "üéâ Your 90%+ accuracy system will work once the LLM is properly trained!"