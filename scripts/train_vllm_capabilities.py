#!/usr/bin/env python3
"""
vLLM Fine-tuning Script for Capabilities Field Training
Trains the Qwen model to properly populate the capabilities field in JSON responses
"""

import json
import asyncio
import httpx
import time
from pathlib import Path
from typing import List, Dict, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VLLMTrainer:
    def __init__(self, base_url: str = "http://localhost:8007/v1"):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=120.0)
        
    async def test_connection(self) -> bool:
        """Test if vLLM is responding"""
        try:
            response = await self.client.get(f"{self.base_url}/models")
            return response.status_code == 200
        except Exception as e:
            logger.error(f"Connection test failed: {e}")
            return False
    
    async def fine_tune_with_training_data(self, training_file: str) -> None:
        """
        Fine-tune the model using the training data via continuous inference
        This simulates training by repeatedly calling the model with examples
        """
        logger.info(f"Loading training data from {training_file}")
        
        with open(training_file, 'r') as f:
            training_data = json.load(f)
        
        logger.info(f"Loaded {len(training_data)} training examples")
        
        # Test the model BEFORE training
        await self.test_critical_example("before_training")
        
        # Process training examples in batches
        batch_size = 10
        total_batches = len(training_data) // batch_size
        
        for batch_idx in range(0, len(training_data), batch_size):
            batch = training_data[batch_idx:batch_idx + batch_size]
            batch_num = batch_idx // batch_size + 1
            
            logger.info(f"Processing batch {batch_num}/{total_batches}")
            
            # Process each example in the batch
            tasks = []
            for example in batch:
                task = self.train_on_example(example)
                tasks.append(task)
            
            # Execute batch concurrently
            await asyncio.gather(*tasks, return_exceptions=True)
            
            # Brief pause between batches
            await asyncio.sleep(0.5)
            
            # Test progress every 10 batches
            if batch_num % 10 == 0:
                await self.test_critical_example(f"batch_{batch_num}")
        
        # Test the model AFTER training
        await self.test_critical_example("after_training")
        
        logger.info("Training completed!")
    
    async def train_on_example(self, example: Dict[str, Any]) -> None:
        """Train on a single example by making inference calls"""
        try:
            user_request = example["input"]
            expected_output = example["output"]
            
            # Create training prompt that teaches the model the expected behavior
            training_prompt = f"""You are an AI assistant that classifies user requests into structured JSON responses.

User Request: "{user_request}"

Expected Response: {json.dumps(expected_output, indent=2)}

Now classify this request and respond with the exact JSON format above. Pay special attention to the capabilities field - it must NEVER be empty for actionable requests.

User Request: "{user_request}"

Response:"""

            # Make inference call to reinforce the pattern
            payload = {
                "model": "Qwen/Qwen2.5-7B-Instruct-AWQ",
                "messages": [
                    {"role": "user", "content": training_prompt}
                ],
                "max_tokens": 200,
                "temperature": 0.1
            }
            
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                json=payload
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result["choices"][0]["message"]["content"]
                
                # Optionally log discrepancies for analysis
                if "capabilities" not in generated_text or '"capabilities": []' in generated_text:
                    logger.warning(f"Model still generating empty capabilities for: {user_request[:50]}...")
            
        except Exception as e:
            logger.error(f"Error training on example: {e}")
    
    async def test_critical_example(self, phase: str) -> None:
        """Test the critical hostname example that was causing issues"""
        critical_request = "Display contents of /etc/hostname"
        
        prompt = """You are an AI assistant for an IT operations platform. Analyze user requests and return a JSON response with this exact format:

{
  "category": "information|action|monitoring|assistance", 
  "action": "provide_information|execute_command|monitor_system|assist_user",
  "confidence": 0.0-1.0,
  "capabilities": ["system_info", "asset_management", etc]
}

CRITICAL: The capabilities field must NEVER be empty for actionable requests. File reading requests require "system_info" capability.

User request: "Display contents of /etc/hostname"

JSON response:"""

        payload = {
            "model": "Qwen/Qwen2.5-7B-Instruct-AWQ",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 200,
            "temperature": 0.1
        }
        
        try:
            response = await self.client.post(f"{self.base_url}/chat/completions", json=payload)
            result = response.json()
            generated_text = result["choices"][0]["message"]["content"]
            
            logger.info(f"=== CRITICAL TEST ({phase}) ===")
            logger.info(f"Request: {critical_request}")
            logger.info(f"Response: {generated_text}")
            
            # Check if capabilities field is properly populated
            if '"capabilities": []' in generated_text or '"capabilities":[]' in generated_text:
                logger.error("‚ùå STILL GENERATING EMPTY CAPABILITIES!")
            elif '"system_info"' in generated_text:
                logger.info("‚úÖ CAPABILITIES PROPERLY POPULATED!")
            else:
                logger.warning("‚ö†Ô∏è  Response unclear - check manually")
            
            logger.info("=" * 50)
            
        except Exception as e:
            logger.error(f"Critical test failed: {e}")

async def main():
    trainer = VLLMTrainer()
    
    # Test connection
    if not await trainer.test_connection():
        logger.error("Cannot connect to vLLM server. Is it running on port 8007?")
        return
    
    logger.info("‚úÖ Connected to vLLM server")
    
    # Use the 5K training dataset (good balance of coverage and speed)
    training_file = "/home/opsconductor/opsconductor-ng/training_data/training_data_5k.json"
    
    if not Path(training_file).exists():
        logger.error(f"Training file not found: {training_file}")
        return
    
    # Start training
    logger.info("üöÄ Starting vLLM fine-tuning process...")
    await trainer.fine_tune_with_training_data(training_file)
    
    logger.info("üéâ Training completed! Your 90%+ accuracy tool selection should now work!")

if __name__ == "__main__":
    asyncio.run(main())