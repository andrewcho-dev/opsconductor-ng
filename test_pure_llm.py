#!/usr/bin/env python3

import requests
import json

def test_pure_llm_chat():
    """Test the pure LLM conversation system"""
    
    # AI Brain service endpoint
    url = "http://localhost:3005/ai/chat"
    
    # Test message
    test_message = "Hello! Can you help me set up monitoring for my web servers?"
    
    payload = {
        "message": test_message,
        "user_id": 1
    }
    
    print(f"ğŸš€ Testing Pure LLM Chat System")
    print(f"ğŸ“¤ Sending message: {test_message}")
    print(f"ğŸ”— URL: {url}")
    print("-" * 60)
    
    try:
        response = requests.post(url, json=payload, timeout=30)
        
        print(f"ğŸ“Š Status Code: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… SUCCESS!")
            print(f"ğŸ¤– AI Response: {data.get('response', 'No response')}")
            print(f"ğŸ†” Conversation ID: {data.get('data', {}).get('conversation_id', 'None')}")
            print(f"ğŸ¯ Success: {data.get('success', False)}")
            
            # Check if it's using the LLM conversation handler
            if 'llm_conversation_handler' in str(data):
                print(f"ğŸ‰ CONFIRMED: Using Pure LLM Conversation Handler!")
            else:
                print(f"âš ï¸  Response doesn't indicate LLM handler usage")
                
        else:
            print(f"âŒ FAILED!")
            print(f"ğŸ“„ Response: {response.text}")
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ REQUEST FAILED: {e}")
    except Exception as e:
        print(f"âŒ ERROR: {e}")

if __name__ == "__main__":
    test_pure_llm_chat()