FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

# Install Python and dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install vLLM
RUN pip3 install --no-cache-dir vllm==0.11.0

# Pre-download the model (optional but recommended)
RUN pip3 install --no-cache-dir huggingface-hub && \
    python3 -c "from huggingface_hub import snapshot_download; snapshot_download('Qwen/Qwen2.5-7B-Instruct-AWQ')"

# Expose vLLM port
EXPOSE 8000

# Start vLLM server with optimized settings for long context on 12GB GPU
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", "--model", "Qwen/Qwen2.5-7B-Instruct-AWQ", "--quantization", "awq", "--kv-cache-dtype", "auto", "--max-model-len", "16384", "--max-num-seqs", "2", "--gpu-memory-utilization", "0.88", "--enforce-eager", "--dtype", "auto", "--port", "8000", "--host", "0.0.0.0"]