FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

# Install Python and dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install vLLM
RUN pip3 install --no-cache-dir vllm==0.11.0

# Pre-download the model (optional but recommended)
RUN pip3 install --no-cache-dir huggingface-hub && \
    python3 -c "from huggingface_hub import snapshot_download; snapshot_download('Qwen/Qwen2.5-7B-Instruct-AWQ')"

# Expose vLLM port
EXPOSE 8000

# Start vLLM server with optimized settings for long context on 12GB GPU
# Profile: Long window, low concurrency (optimized for complex single-user workflows)
# - FP8 KV cache: Halves KV memory vs FP16, enables longer context
# - max-num-seqs 2: Low concurrency for maximum context window
# - max-model-len 12288: Realistic usable context with room for output
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "Qwen/Qwen2.5-7B-Instruct-AWQ", \
     "--quantization", "awq", \
     "--kv-cache-dtype", "fp8", \
     "--max-model-len", "12288", \
     "--max-num-seqs", "2", \
     "--gpu-memory-utilization", "0.88", \
     "--enforce-eager", \
     "--dtype", "auto", \
     "--port", "8000", \
     "--host", "0.0.0.0"]